{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Delaunay-I/image_cap_generator/blob/main/cap_gen_v3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avVueXtlCcIV",
        "outputId": "2080496d-0564-4688-88f3-2029cc363a27"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2iYRwSDnkOa",
        "outputId": "6046d33e-2ed6-452d-ac02-3fa2a0cb2d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab files\n",
            "/content/drive/MyDrive/colab files\n"
          ]
        }
      ],
      "source": [
        "## for google colab runs\n",
        "%cd /content/drive/MyDrive/colab\\ files\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6Xkro5uPCo_",
        "outputId": "4485f91f-bdf2-4e5b-a0f6-24ed83124b74",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.10.12 (main, Jun  7 2023, 12:45:35) [GCC 9.4.0]\n",
            "keras version 2.12.0\n",
            "tensorflow version 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import keras\n",
        "import sys, time, os, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "print(\"python {}\".format(sys.version))\n",
        "print(\"keras version {}\".format(keras.__version__)); del keras\n",
        "print(\"tensorflow version {}\".format(tf.__version__))\n",
        "\n",
        "def set_seed(sd=123):\n",
        "    from numpy.random import seed\n",
        "    from tensorflow import set_random_seed\n",
        "    import random as rn\n",
        "    ## numpy random seed\n",
        "    seed(sd)\n",
        "    ## core python's random number\n",
        "    rn.seed(sd)\n",
        "    ## tensor flow's random number\n",
        "    set_random_seed(sd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Desired image dimensions\n",
        "IMAGE_SIZE = (75, 75)\n",
        "\n",
        "# Vocabulary size\n",
        "VOCAB_SIZE = 4000\n",
        "\n",
        "# Fixed length allowed for any sequence\n",
        "SEQ_LENGTH = 25\n",
        "\n",
        "# Dimension for the image embeddings and token embeddings\n",
        "EMBED_DIM = 512\n",
        "\n",
        "# Per-layer units in the feed-forward network\n",
        "FF_DIM = 512\n",
        "\n",
        "# Other training parameters\n",
        "BATCH_SIZE = 2\n",
        "SHUFFLE_DIM = 1\n",
        "EPOCHS = 30\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "ep-UlUS6y9qG"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_Flickr_jpg = \"/content/drive/MyDrive/colab files/flickr8k/Images\"\n",
        "\n",
        "dir_Flickr_text = \"/content/drive/MyDrive/colab files/flickr8k/captions.txt\"\n",
        "\n",
        "jpgs = os.listdir(dir_Flickr_jpg)\n",
        "\n",
        "df_txt = pd.read_csv(dir_Flickr_text, skiprows=1, names=[\"filename\", \"caption\"])\n",
        "df_txt['caption'] = df_txt['caption'].str.lower()\n",
        "\n",
        "df_txt['index'] = df_txt.groupby(\"filename\").cumcount()\n",
        "\n",
        "uni_filenames = np.unique(df_txt.filename.values)"
      ],
      "metadata": {
        "id": "DC1apSjzScao"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "tags": [],
        "id": "Xp-X0T5YsTnd"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# Remove punctuations..\n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(str.maketrans('', '', string.punctuation))\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "# Remove a single character word..\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "# Remove words with numeric values..\n",
        "def remove_numeric(text,printTF=False):\n",
        "    text_no_numeric = \"\"\n",
        "    for word in text.split():\n",
        "        isalpha = word.isalpha()\n",
        "        if printTF:\n",
        "            print(\"    {:10} : {:}\".format(word,isalpha))\n",
        "        if isalpha:\n",
        "            text_no_numeric += \" \" + word\n",
        "    return(text_no_numeric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpx1Se0ssTnf",
        "outputId": "37f71751-835b-4378-91e1-12b09e5b01a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-51-0ca6825e3553>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_txt[\"caption\"].iloc[i] = newcaption\n"
          ]
        }
      ],
      "source": [
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    text = remove_numeric(text)\n",
        "    return(text)\n",
        "\n",
        "\n",
        "for i, caption in enumerate(df_txt.caption.values):\n",
        "    newcaption = text_clean(caption)\n",
        "    df_txt[\"caption\"].iloc[i] = newcaption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX1nRou0sTnh",
        "outputId": "e0a058f5-25c1-45c4-e78b-9b139f627ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 8763\n"
          ]
        }
      ],
      "source": [
        "def df_word(df_txt):\n",
        "    vocabulary = []\n",
        "    for txt in df_txt.caption.values:\n",
        "        vocabulary.extend(txt.split())\n",
        "    print('Vocabulary Size: %d' % len(set(vocabulary)))\n",
        "    ct = Counter(vocabulary)\n",
        "    dfword = pd.DataFrame({\"word\":ct.keys(),\"count\":ct.values()})\n",
        "    dfword = dfword.sort_values(by=\"count\",ascending=False)\n",
        "    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n",
        "    return(dfword)\n",
        "dfword = df_word(df_txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEq8DiKoR_Y"
      },
      "source": [
        "# Data prepration\n",
        "prepare text and image separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "HMihehhY6Ps7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from copy import copy\n",
        "def add_start_end_seq_token(captions):\n",
        "    caps = []\n",
        "    for txt in captions:\n",
        "        txt = 'startseq ' + txt + ' endseq'\n",
        "        caps.append(txt)\n",
        "    return(caps)\n",
        "\n",
        "df_txt[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfZe4SJDGlo"
      },
      "source": [
        "# split the dataset int train and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "tags": [],
        "id": "5O5GhQtxsTnm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# create a list of unique image file names in your DataFrame (df_txt) using the unique method of pandas:\n",
        "unique_files = df_txt['filename'].unique()\n",
        "\n",
        "# Split the list of unique file names into train and test sets using the train_test_split function from scikit-learn:\n",
        "train_files, test_files = train_test_split(unique_files, test_size=0.2, random_state=42)\n",
        "\n",
        "# Filter the original DataFrame to include only the rows corresponding to the image files in the train and test sets:\n",
        "train_df = df_txt[df_txt['filename'].isin(train_files)]\n",
        "test_df = df_txt[df_txt['filename'].isin(test_files)]\n",
        "\n",
        "# Verify that there is no leakage by checking if there are any image file names that appear in both the train and test sets:\n",
        "assert len(set(train_df['filename']).intersection(set(test_df['filename']))) == 0\n",
        "assert train_df.shape[0]/5 == train_df.filename.unique().size\n",
        "assert test_df.shape[0]/5 == test_df.filename.unique().size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = train_df.groupby('filename')['caption'].apply(list)\n",
        "train_cap_list = [captions for captions in grouped]\n",
        "\n",
        "grouped = test_df.groupby('filename')['caption'].apply(list)\n",
        "test_cap_list = [captions for captions in grouped]"
      ],
      "metadata": {
        "id": "NqOs1AAIQs-g"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3L06Z6A8aSl"
      },
      "source": [
        "# Image prepration\n",
        "## create features for image using InceptionV3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "O2ZHDosMOXWn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "\n",
        "train_path = dir_Flickr_jpg\n",
        "path_all_images = glob.glob(train_path + '/*jpg')\n",
        "\n",
        "train_img = []  # list of all images in training set\n",
        "test_img = []\n",
        "for im in path_all_images:\n",
        "    file_name = os.path.basename(os.path.normpath(im))\n",
        "    # include images that only exist in the target directory\n",
        "    # can split the dataset this way\n",
        "    if(file_name in train_df.filename.to_list()):\n",
        "        train_img.append(im)\n",
        "    elif (file_name in test_df.filename.to_list()):\n",
        "        test_img.append(im)\n",
        "    else:\n",
        "        print(f\"{file_name} not in the directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess image"
      ],
      "metadata": {
        "id": "UnMowBXcOwCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "def read_image():\n",
        "    def decode_image(image_path):\n",
        "        img = tf.io.read_file(image_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, (75, 75))\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "        return img\n",
        "\n",
        "    return decode_image"
      ],
      "metadata": {
        "id": "a4OIiScJOiKR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAoKpyScvQXD"
      },
      "source": [
        "# Tokenize the captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "1YJRJTOOneuv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "tokenizer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        ")\n",
        "\n",
        "tokenizer.adapt(train_df.caption.to_list())\n",
        "VOCAB_SIZE = len(tokenizer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make dataset"
      ],
      "metadata": {
        "id": "NtstwO0sXbf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(image_path, captions, tokenizer):\n",
        "    read_image_fx = read_image()\n",
        "    img_dataset = tf.data.Dataset.from_tensor_slices(image_path)\n",
        "\n",
        "    img_dataset = (img_dataset\n",
        "                   .map(read_image_fx, num_parallel_calls=AUTOTUNE))\n",
        "\n",
        "    cap_dataset = tf.data.Dataset.from_tensor_slices(captions).map(tokenizer, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    dataset = tf.data.Dataset.zip((img_dataset, cap_dataset))\n",
        "    dataset = dataset.batch(BATCH_SIZE).shuffle(SHUFFLE_DIM).prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = make_dataset(train_img, train_cap_list, tokenizer=tokenizer)\n",
        "valid_dataset = make_dataset(test_img, test_cap_list, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "0OpAQAplP8RQ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataset))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKQdS5CL2_GU",
        "outputId": "9d110ec2-3de8-41c6-a374-27fcd9af1414"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 75, 75, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img, txt = next(iter(train_dataset))\n"
      ],
      "metadata": {
        "id": "WaG_VwOrS6bn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL5xf0ehChWB"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cnn model"
      ],
      "metadata": {
        "id": "PicV7x4iudQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import inception_v3\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_cnn_model():\n",
        "    base_model = inception_v3.InceptionV3(\n",
        "        input_shape=(*IMAGE_SIZE, 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "        )\n",
        "\n",
        "    # Freeze feature extractor layers\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out) # shape: (batch_size, 1, max_image_feats=2048 for inceptionV3)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "\n",
        "    return cnn_model"
      ],
      "metadata": {
        "id": "sE2JUTAd3z6n"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define transformer components"
      ],
      "metadata": {
        "id": "LpMM2ptEJglL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=d_model)\n",
        "\n",
        "        self.token_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=d_model,\n",
        "            mask_zero=True)\n",
        "\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, seq):\n",
        "        seq = self.token_embedding(seq) # (batch, seq, d_model)\n",
        "\n",
        "        x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "        x = x[tf.newaxis, :]  # (1, seq)\n",
        "        x = self.pos_embedding(x)  # (1, seq, d_model)\n",
        "\n",
        "        return self.add([seq,x])"
      ],
      "metadata": {
        "id": "TRtX2VndJy7G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention sub-layers"
      ],
      "metadata": {
        "id": "oJDcsL5tLmVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "class CausalSelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        attn_output = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            use_causal_mask = True)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GlobalSelfAttention(BaseAttention):\n",
        "    def call(self, x, training, mask=None, **kwargs):\n",
        "        attn_output = self.mha(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            attention_mask=None,\n",
        "            training=training,\n",
        "            )\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "    def call(self, x, context, **kwargs):\n",
        "        attn_output, attn_scores = self.mha(\n",
        "            query=x,\n",
        "            key=context,\n",
        "            value=context,\n",
        "            return_attention_scores=True)\n",
        "\n",
        "        # Cache the attention scores for plotting later.\n",
        "        self.last_attn_scores = attn_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "wPKaGB2CF7Sn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "feed-forward sub-layer"
      ],
      "metadata": {
        "id": "VgdsDIjwL2KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.seq = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.add([x, self.seq(x)])\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2jjMfLd5Ham-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Block\n",
        "Encoder layer"
      ],
      "metadata": {
        "id": "HGnT1vFTMwsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, num_layers, embed_dim, dense_dim, num_heads,\n",
        "                dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.dense_proj = layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "        self.enc_layers = [\n",
        "            GlobalSelfAttention(\n",
        "                num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate\n",
        "                )\n",
        "            for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # `inputs` is token-IDs shape: (batch, 1, seq_len)\n",
        "        inputs = self.layernorm_1(inputs)\n",
        "        inputs = self.dense_proj(inputs)\n",
        "        # Add dropout.\n",
        "        x = self.dropout(inputs)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x  # Shape `(batch_size, 1, embed_dim)`.\n"
      ],
      "metadata": {
        "id": "Nf5EdEb4NdRv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer decoder"
      ],
      "metadata": {
        "id": "NWJQp8DbhaND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "\n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):"
      ],
      "metadata": {
        "id": "TBWMoQMgH_LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1MG3hbfsToC"
      },
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HRJYVLJWsToD"
      },
      "outputs": [],
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, image_feats, max_length):\n",
        "    image_feats = image_feats.reshape(1,-1)\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
        "        # predict next word\n",
        "        yhat = model.predict([image_feats, sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQnHtzf5tnjL",
        "tags": []
      },
      "source": [
        "## BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ADDUi-LJsToD"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, df, image_data, tokenizer, max_length):\n",
        "    with mlflow.start_run():\n",
        "\n",
        "        actual, predicted = list(), list()\n",
        "        count = 0\n",
        "        # step over the whole set\n",
        "        for key, image_feats in image_data.items():\n",
        "            count += 1\n",
        "            if count % 200 == 0:\n",
        "                print(\"  {:4.2f}% is done..\".format(100*count/float(df.shape[0]/5)))\n",
        "            # generate description\n",
        "            yhat = generate_desc(model, tokenizer, image_feats, max_length)\n",
        "            # append all the captions of a image file to a list\n",
        "            caption_list = list()\n",
        "            for desc in test_df.loc[df[\"filename\"] == key, \"caption\"]:\n",
        "                caption_list.append(desc)\n",
        "            # store actual and predicted\n",
        "            references = [d.split() for d in caption_list]\n",
        "            actual.append(references)\n",
        "            predicted.append(yhat.split())\n",
        "        # calculate BLEU score\n",
        "        bleu1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "        bleu2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "        bleu3 = corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
        "        bleu4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "        print('BLEU-1: %f' % bleu1)\n",
        "        print('BLEU-2: %f' % bleu2)\n",
        "        print('BLEU-3: %f' % bleu3)\n",
        "        print('BLEU-4: %f' % bleu4)\n",
        "\n",
        "        mlflow.log_metric(\"BLEU-1\", bleu1)\n",
        "        mlflow.log_metric(\"BLEU-2\", bleu2)\n",
        "        mlflow.log_metric(\"BLEU-3\", bleu3)\n",
        "        mlflow.log_metric(\"BLEU-4\", bleu4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "B9fpnRzbsToE",
        "outputId": "d1d83448-85a7-453d-b211-8657e32c106d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  12.35% is done..\n",
            "  24.71% is done..\n",
            "  37.06% is done..\n",
            "  49.41% is done..\n",
            "  61.77% is done..\n",
            "  74.12% is done..\n",
            "  86.47% is done..\n",
            "  98.83% is done..\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(model, test_df, test_img_feats, tokenizer, max_length)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}