{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6Xkro5uPCo_",
    "outputId": "f483fc8c-8847-4e5a-ef07-ecf5b3fe0e52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.9.16 (main, Mar  8 2023, 14:00:05) \n",
      "[GCC 11.2.0]\n",
      "keras version 2.12.0\n",
      "tensorflow version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys, time, os, warnings \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter \n",
    "\n",
    "print(\"python {}\".format(sys.version))\n",
    "print(\"keras version {}\".format(keras.__version__)); del keras\n",
    "print(\"tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "def set_seed(sd=123):\n",
    "    from numpy.random import seed\n",
    "    from tensorflow import set_random_seed\n",
    "    import random as rn\n",
    "    ## numpy random seed\n",
    "    seed(sd)\n",
    "    ## core python's random number \n",
    "    rn.seed(sd)\n",
    "    ## tensor flow's random number\n",
    "    set_random_seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mu7zkvTxPT-9",
    "outputId": "0ff63aed-e7da-4f3d-a16d-e1f249a796d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./flickr8k\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# !pip install opendatasets\n",
    "import opendatasets as od\n",
    "import pandas\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/adityajn105/flickr8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNw0Wid3Vlpu",
    "outputId": "be4810b0-86f2-4fce-fce2-96a0c0926195",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jpg flies in Flicker8k: 8092\n"
     ]
    }
   ],
   "source": [
    "## The location of the Flickr8K_ photos\n",
    "dir_Flickr_jpg = \"./flickr8k/Images\"\n",
    "## The location of the caption file\n",
    "dir_Flickr_text = \"./flickr8k/captions.txt\"\n",
    "\n",
    "jpgs = os.listdir(dir_Flickr_jpg)\n",
    "print(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45SpOt_GXL-u"
   },
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YVHrZfSYxmO",
    "outputId": "5cb5f94a-4a47-4d00-e468-0b7f4271f559",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique file names : 8091\n",
      "The distribution of the number of captions for each image:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({5: 8091})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txt = pd.read_csv(dir_Flickr_text, skiprows=1, names=[\"filename\", \"caption\"])\n",
    "df_txt['caption'] = df_txt['caption'].str.lower()\n",
    "\n",
    "df_txt['index'] = df_txt.groupby(\"filename\").cumcount()\n",
    "\n",
    "uni_filenames = np.unique(df_txt.filename.values)\n",
    "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
    "print(\"The distribution of the number of captions for each image:\")\n",
    "Counter(Counter(df_txt.filename.values).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OEq8DiKoR_Y"
   },
   "source": [
    "# Data prepration\n",
    "prepare text and image separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HMihehhY6Ps7",
    "outputId": "206f38ca-5c32-4af2-df9f-a989007cb741",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a child in a pink dress is climbing u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a girl going into a wooden building ....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a little girl climbing into a wooden ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a little girl climbing the stairs to ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a little girl in a pink dress going i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename   \n",
       "0  1000268201_693b08cb0e.jpg  \\\n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  index  \n",
       "0  startseq a child in a pink dress is climbing u...      0  \n",
       "1  startseq a girl going into a wooden building ....      1  \n",
       "2  startseq a little girl climbing into a wooden ...      2  \n",
       "3  startseq a little girl climbing the stairs to ...      3  \n",
       "4  startseq a little girl in a pink dress going i...      4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "def add_start_end_seq_token(captions):\n",
    "    caps = []\n",
    "    for txt in captions:\n",
    "        txt = 'startseq ' + txt + ' endseq'\n",
    "        caps.append(txt)\n",
    "    return(caps)\n",
    "\n",
    "df_txt[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
    "df_txt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfZe4SJDGlo"
   },
   "source": [
    "# split the dataset int train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MxqCDgSDDFKw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split your df_txt into train and test sets\n",
    "train_df, test_df = train_test_split(df_txt, test_size=0.2, shuffle=False)\n",
    "# dropping the imagefile that with shared captions in the two splits\n",
    "# this would also avoid data leakage\n",
    "test_df = test_df.iloc[1:]\n",
    "train_df =  train_df[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3L06Z6A8aSl"
   },
   "source": [
    "# Image prepration\n",
    "## create features for image using InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GgffeHrI8d49",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "image_model = Model(inputs = base_model.input, outputs=base_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "O2ZHDosMOXWn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "train_path = dir_Flickr_jpg\n",
    "path_all_images = glob.glob(train_path + '/*jpg')\n",
    "\n",
    "train_img = []  # list of all images in training set\n",
    "test_img = []\n",
    "for im in path_all_images:\n",
    "    file_name = os.path.basename(os.path.normpath(im))\n",
    "    # include images that only exist in the target directory\n",
    "    # can split the dataset this way\n",
    "    if(file_name in train_df.filename.to_list()):\n",
    "        train_img.append(im)\n",
    "    elif (file_name in test_df.filename.to_list()):\n",
    "        test_img.append(im)\n",
    "\n",
    "def preprocess(image_path):\n",
    "    # inception v3 excepts img in 299 * 299 * 3\n",
    "    image = load_img(image_path, target_size=(299, 299))\n",
    "    # convert the image pixels to a numpy array\n",
    "    x = img_to_array(image)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def encode(image_path):\n",
    "    image = preprocess(image_path)\n",
    "    vec = image_model.predict(image, verbose=0)\n",
    "    vec_flattened = vec.flatten()\n",
    "    return vec_flattened\n",
    "\n",
    "\n",
    "train_img_feats = {}\n",
    "test_img_feats = {}\n",
    "\n",
    "if not (os.path.exists('train_encoder.pkl') and os.path.exists('test_encoder.pkl')):\n",
    "    for image in train_img:\n",
    "        file_name = os.path.basename(os.path.normpath(image))\n",
    "        train_img_feats[file_name] = encode(image)\n",
    "    for image in test_img:\n",
    "        file_name = os.path.basename(os.path.normpath(image))\n",
    "        test_img_feats[file_name] = encode(image)\n",
    "    # Save the image features\n",
    "    with open('train_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(train_img_feats, f)\n",
    "    with open('test_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(test_img_feats, f)\n",
    "else:\n",
    "    # Load previously encoded image data\n",
    "    with open('train_encoder.pkl', 'rb') as f:\n",
    "        train_img_feats = pickle.load(f)\n",
    "    with open('test_encoder.pkl', 'rb') as f:\n",
    "        test_img_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAoKpyScvQXD"
   },
   "source": [
    "# Tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 8000\n",
    "# Tokenizer does not limit the number of words\n",
    "# it still finds all the words in the word_index\n",
    "# But it will only use the num_words given to encode the text in texts_to_sequences or sequences_to_texts methods\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df.caption.to_list())\n",
    "\n",
    "# get the word index\n",
    "train_seqs = tokenizer.texts_to_sequences(train_df.caption.to_list())\n",
    "test_seq = tokenizer.texts_to_sequences(test_df.caption.to_list())\n",
    "\n",
    "# calculate the maximum caption length\n",
    "max_length = max(len(seq) for seq in train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic_Utl6I4ev2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def data_generator(df, vectorizer, max_length, vocab_size, image_data, batch_size):\n",
    "    num_batches = len(df) // batch_size\n",
    "    while True:\n",
    "        for i in range(num_batches):\n",
    "            batch_df = df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            X1, X2, y = [], [], []\n",
    "            for index, row in batch_df.iterrows():\n",
    "                try:\n",
    "                    # try to get the image features from the image_data dictionary\n",
    "                    pic = image_data[row['filename']]\n",
    "                except KeyError:\n",
    "                    # if the file name is not found, print a warning message and skip this row\n",
    "                    print(f\"Warning: file name {row['filename']} not found in image_data dictionary. Skipping this row.\")\n",
    "                    continue\n",
    "                seq = vectorizer([row['caption']]).numpy()[0] # convert caption to vectorized tensor and then to numpy array\n",
    "                # seq = np.argmax(seq) # optional: get index of max value in vector\n",
    "                for j in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:j], seq[j]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    X1.append(pic)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "\n",
    "batch_size = 64\n",
    "# create data generator for the train set\n",
    "train_generator = data_generator(df_train, vectorizer, max_length, vocab_size, train_img_feats, batch_size)\n",
    "\n",
    "# create data generator for the test set\n",
    "test_generator = data_generator(df_test, vectorizer, max_length, vocab_size, test_img_feats, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = df_txt[:6]\n",
    "for index, row in test_df.iterrows():\n",
    "    pic = image_data[row['filename']]\n",
    "    print(f\"pic: {pic}, filename: {row['filename']}\")\n",
    "    seq = vectorizer([row['caption']]).numpy()[0]\n",
    "    print(f\"seq: {seq}, seq.shape: {seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_LjvGtNAUUw"
   },
   "source": [
    "# Downloading GloVe to using its vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fddgWxT36kCN",
    "outputId": "445aa7e5-efe3-4493-cc17-eb43ecb71942",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Set the URL for the GloVe embeddings\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "# Set the path where the embeddings will be stored\n",
    "embeddings_dir = 'embeddings/glove'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(embeddings_dir):\n",
    "    os.makedirs(embeddings_dir)\n",
    "\n",
    "# Set the file name for the embeddings archive\n",
    "embeddings_zip = os.path.join(embeddings_dir, 'glove.6B.zip')\n",
    "\n",
    "# Download the embeddings archive if it doesn't exist\n",
    "if not os.path.exists(embeddings_zip):\n",
    "    print(f'Downloading GloVe embeddings from {url}...')\n",
    "    urllib.request.urlretrieve(url, embeddings_zip)\n",
    "    print('Done!')\n",
    "\n",
    "# Extract the embeddings if they haven't been extracted yet\n",
    "if not os.path.exists(os.path.join(embeddings_dir, 'glove.6B.100d.txt')):\n",
    "    print('Extracting GloVe embeddings...')\n",
    "    with zipfile.ZipFile(embeddings_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(embeddings_dir)\n",
    "    print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGfT12woASi-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "glove_path = \"./embeddings/glove/glove.6B.200d.txt\"\n",
    "\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# prepare embedding matrix\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for i, word in enumerate(word_list):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL5xf0ehChWB"
   },
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_3NT4N7CjxV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# define the model\n",
    "ip1 = layers.Input(shape = (2048, ))\n",
    "fe1 = layers.Dropout(0.2)(ip1)\n",
    "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
    "ip2 = layers.Input(shape = (max_length, ))\n",
    "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
    "se2 = layers.Dropout(0.2)(se1)\n",
    "se3 = layers.LSTM(256)(se2)\n",
    "decoder1 = layers.add([fe2, se3])\n",
    "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
    "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "model2 = Model(inputs = [ip1, ip2], outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPygHAUZDIyn"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2.layers[2].set_weights([embedding_matrix])\n",
    "model2.layers[2].trainable = False\n",
    "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "num_batches = len(train_df) // batch_size\n",
    "num_val_batches = len(test_df) // batch_size\n",
    "\n",
    "model2.fit(train_generator,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=num_val_batches,\n",
    "          epochs = 50,\n",
    "          steps_per_epoch=num_batches,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "ESmmmVAjDLla",
    "outputId": "088b6c89-fe93-4230-c047-5e0b761007bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "num_batches = len(train_df) // batch_size\n",
    "num_val_batches = len(test_df) // batch_size\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=num_val_batches,\n",
    "          epochs = 50,\n",
    "          steps_per_epoch=num_batches,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inSz8Ua2TM6P",
    "tags": []
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_YvQNHyTDhY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some custom metadata for the model\n",
    "metadata = {\n",
    "  'name': 'image_caption_generator',\n",
    "  'description': 'A model that generates captions for images using InceptionV3 and have all the stop words, and with no lemmatization',\n",
    "  'parameters': {\n",
    "    'vocab_size': 8000,\n",
    "    'embedding_dim': 200,\n",
    "    'lstm_units': 256,\n",
    "    'beam_size': 5\n",
    "  },\n",
    "  'performance': {\n",
    "    'loss': 1.6700,\n",
    "    'accuracy': 'Nan',\n",
    "    'bleu_score': 'NaN'\n",
    "  }\n",
    "}\n",
    "\n",
    "# save the model with the metadata\n",
    "model.save('capGen_model_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D02QybmdJ3F2"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('img_cap_model_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_old = tf.keras.models.load_model(\"./caption_generator_inceptionV3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-BydfZoYSi7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the new image\n",
    "img = encode(\"test_img.jpg\")\n",
    "img = img.reshape((1, 2048))\n",
    "\n",
    "caption = [vectorizer([\"startseq\"]).numpy()[0][0]]\n",
    "\n",
    "for i in range(max_length):\n",
    "    padded_caption = pad_sequences([caption], maxlen=22, padding='post')\n",
    "    prediction = model.predict([img, padded_caption], verbose=0)\n",
    "    word_index = np.argmax(prediction)\n",
    "    caption.append(word_index)\n",
    "    if word_index == vectorizer_word_index[\"endseq\"]:\n",
    "        break\n",
    "\n",
    "\n",
    "caption_words = [vectorizer_index_word[i] for i in caption]\n",
    "# join the words to form a sentence\n",
    "caption_sentence = ' '.join(caption_words[1:-1])\n",
    "caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beam_search_caption(image_path, beam_width):\n",
    "    # encode the image\n",
    "    image_vec = encode(image_path)\n",
    "    image_vec = image_vec.reshape(1, -1)\n",
    "    # initialize the caption with the start token\n",
    "    caption = [vectorizer([\"startseq\"]).numpy()[0][0]]\n",
    "    # initialize beam search\n",
    "    beam = [(caption, 0)]\n",
    "    \n",
    "    # loop until the end token or the maximum length is reached\n",
    "    for i in range(max_length):\n",
    "        # generate new candidates\n",
    "        candidates = []\n",
    "        for j in range(len(beam)):\n",
    "            seq, score = beam[j]\n",
    "            # check if the sequence ends with endseq\n",
    "            if seq[-1] == vectorizer_word_index[\"endseq\"]:\n",
    "                candidates.append((seq, score))\n",
    "                continue\n",
    "            # predict the next word using the model\n",
    "            padded_caption = pad_sequences([seq], maxlen=max_length, padding='post')\n",
    "            prediction = model.predict([image_vec, padded_caption], verbose=0)[0]\n",
    "            # get the top k words with the highest probability\n",
    "            top_k = prediction.argsort()[-beam_width:][::-1]\n",
    "            # add new candidates to the list\n",
    "            for w in top_k:\n",
    "                new_seq = seq + [w]\n",
    "                new_score = score + np.log(prediction[w])\n",
    "                candidates.append((new_seq, new_score))\n",
    "        # select top k candidates\n",
    "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "    # select the best candidate\n",
    "    seq, score = beam[0]\n",
    "    # convert the caption indices to words\n",
    "    caption_words = [vectorizer_index_word[i] for i in seq]\n",
    "    # join the words to form a sentence\n",
    "    caption_sentence = ' '.join(caption_words[1:-1])\n",
    "    return caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d0r5UjhV8Q-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "generated_caption = beam_search_caption(\"test_img.jpg\", 10)\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7owjVv1BmGZb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer([\"hey you what's up\"]).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, row in train_df[2:5].iterrows():\n",
    "    seq = vectorizer(row['caption']).numpy()\n",
    "    print(vectorizer(row['caption']).numpy())\n",
    "    print(row['caption'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cffdeMRmHEb"
   },
   "source": [
    "# Model 2, more complicated\n",
    "* You can use a bidirectional LSTM instead of a single LSTM for the caption encoder. This way, you can capture the context from both directions of the caption sequence, and generate more coherent captions.\n",
    "\n",
    "* You can use an attention mechanism to allow the decoder to focus on different parts of the image and the caption encoder outputs at each time step. This way, you can generate more relevant and informative captions that align with the image content.\n",
    "\n",
    "* You can use a scheduled sampling technique to train the decoder with a mix of ground truth and predicted words. This way, you can reduce the exposure bias and improve the generalization ability of the decoder.\n",
    "\n",
    "* You can use a beam search instead of a greedy search for generating captions. This way, you can explore more possible captions and choose the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93vaiXH4mJx5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "# define the model\n",
    "ip1 = layers.Input(shape = (2048, ))\n",
    "fe1 = layers.Dropout(0.2)(ip1)\n",
    "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
    "ip2 = layers.Input(shape = (max_length, ))\n",
    "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
    "se2 = layers.Dropout(0.2)(se1)\n",
    "se3 = Bidirectional(layers.LSTM(256, return_sequences=True))(se2) # use bidirectional LSTM\n",
    "decoder1 = layers.add([fe2, se3[:, -1]]) # use last hidden state of bidirectional LSTM\n",
    "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
    "attn_layer = Attention() # use attention layer\n",
    "context_vector, attention_weights = attn_layer([decoder2, se3]) # get context vector and attention weights\n",
    "decoder3 = layers.Dense(256, activation='relu')(context_vector) # use context vector for final dense layer\n",
    "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder3)\n",
    "model_v2 = Model(inputs = [ip1, ip2], outputs = outputs)\n",
    "\n",
    "model_v2.layers[3].set_weights([embedding_matrix])\n",
    "model_v2.layers[3].trainable = False\n",
    "model_v2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "num_batches = len(df_txt0) // batch_size\n",
    "model_v2.fit(train_generator, epochs = 50, steps_per_epoch=num_batches, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UXfPrCcmxsq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ6uH3DimysF"
   },
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgdmGhyLmy8M"
   },
   "outputs": [],
   "source": [
    "def beam_search(image_path, beam_size):\n",
    "  # encode the image\n",
    "  image_vec = encode(image_path)\n",
    "  # add another dimension to match the model input\n",
    "  image_vec = np.expand_dims(image_vec, axis=0)\n",
    "  # initialize the candidates with the start token\n",
    "  candidates = [[tokenizer.word_index['<start>']]]\n",
    "  # initialize the probabilities with 1\n",
    "  probabilities = [1]\n",
    "  # loop until the maximum length is reached\n",
    "  for i in range(max_length):\n",
    "    # initialize a list to store the next candidates\n",
    "    next_candidates = []\n",
    "    # initialize a list to store the next probabilities\n",
    "    next_probabilities = []\n",
    "    # loop over the current candidates\n",
    "    for j in range(len(candidates)):\n",
    "      # get the current candidate\n",
    "      candidate = candidates[j]\n",
    "      # pad the candidate sequence\n",
    "      padded_candidate = pad_sequences([candidate], maxlen=max_length, padding='post')\n",
    "      # predict the next word using the model\n",
    "      prediction = model.predict([image_vec, padded_candidate], verbose=0)\n",
    "      # get the top beam_size words and their probabilities\n",
    "      top_words = np.argsort(prediction[0])[-beam_size:]\n",
    "      top_probs = prediction[0][top_words]\n",
    "      # loop over the top words and their probabilities\n",
    "      for k in range(beam_size):\n",
    "        # get the word and its probability\n",
    "        word = top_words[k]\n",
    "        prob = top_probs[k]\n",
    "        # append the word to the candidate and multiply the probability\n",
    "        next_candidate = candidate + [word]\n",
    "        next_prob = probabilities[j] * prob\n",
    "        # append the next candidate and probability to the lists\n",
    "        next_candidates.append(next_candidate)\n",
    "        next_probabilities.append(next_prob)\n",
    "    # sort the next candidates and probabilities by descending order of probability\n",
    "    sorted_indices = np.argsort(next_probabilities)[::-1]\n",
    "    sorted_candidates = [next_candidates[i] for i in sorted_indices]\n",
    "    sorted_probabilities = [next_probabilities[i] for i in sorted_indices]\n",
    "    # select the top beam_size candidates and probabilities for the next iteration\n",
    "    candidates = sorted_candidates[:beam_size]\n",
    "    probabilities = sorted_probabilities[:beam_size]\n",
    "    # check if any candidate has reached the end token\n",
    "    end_index = tokenizer.word_index['<end>']\n",
    "    if any(candidate[-1] == end_index for candidate in candidates):\n",
    "      break\n",
    "  # return the candidate with the highest probability\n",
    "  best_candidate = candidates[0]\n",
    "  # convert the candidate indices to words\n",
    "  caption_words = [tokenizer.index_word[i] for i in best_candidate]\n",
    "  # join the words to form a sentence\n",
    "  caption_sentence = ' '.join(caption_words[1:-1])\n",
    "  return caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSA86kO8z39g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(df, image_data, batch_size):\n",
    "    # create a dataset from the data frame\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df['filename'], df['caption']))\n",
    "    # map the filename to the image data\n",
    "    ds = ds.map(lambda x, y: (image_data[x], y))\n",
    "    # apply the TextVectorization layer as a transformation\n",
    "    ds = ds.map(lambda x, y: (x, vectorizer([y])))\n",
    "    # unbatch the dataset to get individual elements\n",
    "    ds = ds.unbatch()\n",
    "    # create input and output sequences\n",
    "    ds = ds.map(lambda x, y: (x, y[:-1], y[1:]))\n",
    "    # pad and one-hot encode the sequences if needed\n",
    "    # ds = ds.map(lambda x, y, z: (x, pad_sequences([y], maxlen=max_length)[0], to_categorical([z], num_classes=vocab_size)[0]))\n",
    "    # batch the dataset\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = data_generator(df_txt, image_data, batch_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
