{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2iYRwSDnkOa",
    "outputId": "14afe5dd-17cf-4562-af5d-d7c2ee97ff4b"
   },
   "outputs": [],
   "source": [
    "# for google colab runs\n",
    "# %cd /content/drive/MyDrive/colab\\ files\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6Xkro5uPCo_",
    "outputId": "14c445ab-244d-406b-bb1c-3f83fdc2f5fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys, time, os, warnings \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter \n",
    "\n",
    "print(\"python {}\".format(sys.version))\n",
    "print(\"keras version {}\".format(keras.__version__)); del keras\n",
    "print(\"tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "def set_seed(sd=123):\n",
    "    from numpy.random import seed\n",
    "    from tensorflow import set_random_seed\n",
    "    import random as rn\n",
    "    ## numpy random seed\n",
    "    seed(sd)\n",
    "    ## core python's random number \n",
    "    rn.seed(sd)\n",
    "    ## tensor flow's random number\n",
    "    set_random_seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mu7zkvTxPT-9",
    "outputId": "c635a74f-9a0c-42d7-e4d7-b9355f744746",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install opendatasets\n",
    "import opendatasets as od\n",
    "import pandas\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/adityajn105/flickr8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNw0Wid3Vlpu",
    "outputId": "d04fdfd6-ecef-4320-b0aa-353bf36466ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The location of the Flickr8K_ photos\n",
    "dir_Flickr_jpg = \"./flickr8k/Images\"\n",
    "# dir_Flickr_jpg = \"/content/drive/MyDrive/colab files/flickr8k/Images\"\n",
    "\n",
    "## The location of the caption file\n",
    "dir_Flickr_text = \"./flickr8k/captions.txt\"\n",
    "# dir_Flickr_text = \"/content/drive/MyDrive/colab files/flickr8k/captions.txt\"\n",
    "\n",
    "jpgs = os.listdir(dir_Flickr_jpg)\n",
    "print(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45SpOt_GXL-u"
   },
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YVHrZfSYxmO",
    "outputId": "d0f58279-a28c-4f5e-aae6-da0b15b711d7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_txt = pd.read_csv(dir_Flickr_text, skiprows=1, names=[\"filename\", \"caption\"])\n",
    "df_txt['caption'] = df_txt['caption'].str.lower()\n",
    "\n",
    "df_txt['index'] = df_txt.groupby(\"filename\").cumcount()\n",
    "\n",
    "uni_filenames = np.unique(df_txt.filename.values)\n",
    "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
    "print(\"The distribution of the number of captions for each image:\")\n",
    "Counter(Counter(df_txt.filename.values).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Remove punctuations..\n",
    "def remove_punctuation(text_original):\n",
    "    text_no_punctuation = text_original.translate(str.maketrans('', '', string.punctuation))\n",
    "    return(text_no_punctuation)\n",
    "\n",
    "# Remove a single character word..\n",
    "def remove_single_character(text):\n",
    "    text_len_more_than1 = \"\"\n",
    "    for word in text.split():\n",
    "        if len(word) > 1:\n",
    "            text_len_more_than1 += \" \" + word\n",
    "    return(text_len_more_than1)\n",
    "\n",
    "# Remove words with numeric values..\n",
    "def remove_numeric(text,printTF=False):\n",
    "    text_no_numeric = \"\"\n",
    "    for word in text.split():\n",
    "        isalpha = word.isalpha()\n",
    "        if printTF:\n",
    "            print(\"    {:10} : {:}\".format(word,isalpha))\n",
    "        if isalpha:\n",
    "            text_no_numeric += \" \" + word\n",
    "    return(text_no_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_clean(text_original):\n",
    "    text = remove_punctuation(text_original)\n",
    "    text = remove_single_character(text)\n",
    "    text = remove_numeric(text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "for i, caption in enumerate(df_txt.caption.values):\n",
    "    newcaption = text_clean(caption)\n",
    "    df_txt[\"caption\"].iloc[i] = newcaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_word(df_txt):\n",
    "    vocabulary = []\n",
    "    for txt in df_txt.caption.values:\n",
    "        vocabulary.extend(txt.split())\n",
    "    print('Vocabulary Size: %d' % len(set(vocabulary)))\n",
    "    ct = Counter(vocabulary)\n",
    "    dfword = pd.DataFrame({\"word\":ct.keys(),\"count\":ct.values()})\n",
    "    dfword = dfword.sort_values(by=\"count\",ascending=False)\n",
    "    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n",
    "    return(dfword)\n",
    "dfword = df_word(df_txt)\n",
    "dfword.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topn = 50\n",
    "\n",
    "def plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n",
    "    plt.figure(figsize=(20,3))\n",
    "    plt.bar(dfsub.index,dfsub[\"count\"])\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "plthist(dfword.iloc[:topn,:],\n",
    "        title=\"The top 50 most frequently appearing words\")\n",
    "plthist(dfword.iloc[-topn:,:],\n",
    "        title=\"The least 50 most frequently appearing words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OEq8DiKoR_Y"
   },
   "source": [
    "# Data prepration\n",
    "prepare text and image separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HMihehhY6Ps7",
    "outputId": "0f055661-dac9-469d-a1a8-d351e741ae6a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "def add_start_end_seq_token(captions):\n",
    "    caps = []\n",
    "    for txt in captions:\n",
    "        txt = 'startseq ' + txt + ' endseq'\n",
    "        caps.append(txt)\n",
    "    return(caps)\n",
    "\n",
    "df_txt[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
    "df_txt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfZe4SJDGlo"
   },
   "source": [
    "# split the dataset int train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a list of unique image file names in your DataFrame (df_txt) using the unique method of pandas:\n",
    "unique_files = df_txt['filename'].unique()\n",
    "\n",
    "# Split the list of unique file names into train and test sets using the train_test_split function from scikit-learn:\n",
    "train_files, test_files = train_test_split(unique_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filter the original DataFrame to include only the rows corresponding to the image files in the train and test sets:\n",
    "train_df = df_txt[df_txt['filename'].isin(train_files)]\n",
    "test_df = df_txt[df_txt['filename'].isin(test_files)]\n",
    "\n",
    "# Verify that there is no leakage by checking if there are any image file names that appear in both the train and test sets:\n",
    "assert len(set(train_df['filename']).intersection(set(test_df['filename']))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3L06Z6A8aSl"
   },
   "source": [
    "# Image prepration\n",
    "## create features for image using InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgffeHrI8d49",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "image_model = Model(inputs = base_model.input, outputs=base_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2ZHDosMOXWn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "train_path = dir_Flickr_jpg\n",
    "path_all_images = glob.glob(train_path + '/*jpg')\n",
    "\n",
    "train_img = []  # list of all images in training set\n",
    "test_img = []\n",
    "for im in path_all_images:\n",
    "    file_name = os.path.basename(os.path.normpath(im))\n",
    "    # include images that only exist in the target directory\n",
    "    # can split the dataset this way\n",
    "    if(file_name in train_df.filename.to_list()):\n",
    "        train_img.append(im)\n",
    "    elif (file_name in test_df.filename.to_list()):\n",
    "        test_img.append(im)\n",
    "    else:\n",
    "        print(f\"{file_name} not in the directory\")\n",
    "    \n",
    "\n",
    "def preprocess(image_path):\n",
    "    # inception v3 excepts img in 299 * 299 * 3\n",
    "    image = load_img(image_path, target_size=(299, 299))\n",
    "    # convert the image pixels to a numpy array\n",
    "    x = img_to_array(image)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def encode(image_path):\n",
    "    image = preprocess(image_path)\n",
    "    vec = image_model.predict(image, verbose=0)\n",
    "    vec_flattened = vec.flatten()\n",
    "    return vec_flattened\n",
    "\n",
    "\n",
    "train_img_feats = {}\n",
    "test_img_feats = {}\n",
    "\n",
    "if not (os.path.exists('train_encoder.pkl') and os.path.exists('test_encoder.pkl')):\n",
    "    for image in train_img:\n",
    "        file_name = os.path.basename(os.path.normpath(image))\n",
    "        train_img_feats[file_name] = encode(image)\n",
    "    for image in test_img:\n",
    "        file_name = os.path.basename(os.path.normpath(image))\n",
    "        test_img_feats[file_name] = encode(image)\n",
    "    # Save the image features\n",
    "    with open('train_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(train_img_feats, f)\n",
    "    with open('test_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(test_img_feats, f)\n",
    "else:\n",
    "    # Load previously encoded image data\n",
    "    with open('train_encoder.pkl', 'rb') as f:\n",
    "        train_img_feats = pickle.load(f)\n",
    "    with open('test_encoder.pkl', 'rb') as f:\n",
    "        test_img_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAoKpyScvQXD"
   },
   "source": [
    "# Tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YJRJTOOneuv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Tokenizer does not limit the number of words\n",
    "# it still finds all the words in the word_index\n",
    "# But it will only use the num_words given to encode the text in texts_to_sequences or sequences_to_texts methods\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df.caption.to_list())\n",
    "\n",
    "# Add 1 to the vocab size to account for the 0 index used for padding sequences\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# get the word index\n",
    "train_seqs = tokenizer.texts_to_sequences(train_df.caption.to_list())\n",
    "test_seqs = tokenizer.texts_to_sequences(test_df.caption.to_list())\n",
    "\n",
    "# calculate the maximum caption length\n",
    "max_length = max(len(seq) for seq in train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic_Utl6I4ev2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def data_generator(df, tokenizer, max_length, image_data, batch_size, generator_type):\n",
    "    num_batches = len(df) // batch_size\n",
    "    while True:\n",
    "        df = df.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "        for i in range(num_batches):\n",
    "            batch_df = df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            X1, X2, y = [], [], []\n",
    "            for index, row in batch_df.iterrows():\n",
    "                try:\n",
    "                    # try to get the image features from the image_data dictionary\n",
    "                    pic = image_data[row['filename']]\n",
    "                except KeyError:\n",
    "                    # if the file name is not found, print a warning message and skip this row\n",
    "                    print(f\"\"\"Warning ({generator_type} generator):\n",
    "                          file name {row['filename']} not found in image_data dictionary. Skipping this row.\"\"\")\n",
    "                    continue\n",
    "                seq = tokenizer.texts_to_sequences([row['caption']])[0]\n",
    "                for j in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:j], seq[j]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=len(tokenizer.word_index)+1)[0]\n",
    "                    # out_seq = [out_seq]\n",
    "                    X1.append(pic)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            yield ([np.array(X1), np.array(X2)], np.array(y))\n",
    "\n",
    "batch_size = 64\n",
    "# create data generator for the train set\n",
    "train_generator = data_generator(train_df, tokenizer, max_length, train_img_feats, batch_size, generator_type=\"train\")\n",
    "\n",
    "# create data generator for the test set\n",
    "test_generator = data_generator(test_df, tokenizer, max_length, test_img_feats, batch_size, generator_type=\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_LjvGtNAUUw"
   },
   "source": [
    "# Downloading GloVe to using its vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fddgWxT36kCN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Set the URL for the GloVe embeddings\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "# Set the path where the embeddings will be stored\n",
    "embeddings_dir = 'embeddings/glove'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(embeddings_dir):\n",
    "    os.makedirs(embeddings_dir)\n",
    "\n",
    "# Set the file name for the embeddings archive\n",
    "embeddings_zip = os.path.join(embeddings_dir, 'glove.6B.zip')\n",
    "\n",
    "# Download the embeddings archive if it doesn't exist\n",
    "if not os.path.exists(embeddings_zip):\n",
    "    print(f'Downloading GloVe embeddings from {url}...')\n",
    "    urllib.request.urlretrieve(url, embeddings_zip)\n",
    "    print('Done!')\n",
    "\n",
    "# Extract the embeddings if they haven't been extracted yet\n",
    "if not os.path.exists(os.path.join(embeddings_dir, 'glove.6B.100d.txt')):\n",
    "    print('Extracting GloVe embeddings...')\n",
    "    with zipfile.ZipFile(embeddings_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(embeddings_dir)\n",
    "    print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGfT12woASi-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "glove_path = \"./embeddings/glove/glove.6B.200d.txt\"\n",
    "\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# filter out the words that are not in the top num_words-1 most frequent words\n",
    "# tokenizer numbers words in the word_index by most repeated\n",
    "# filtered_word_index = {word: i for word, i in tokenizer.word_index.items() if i <= vocab_size}\n",
    "# assert len(filtered_word_index) >= vocab_size, f\"\"\"the specified vocab size is larger than the tokenizer word_index.\n",
    "# vocab_size={vocab_size}, length of tokenizer.word_index={len(filtered_word_index)}\n",
    "# Use smaller vocab size\"\"\"\n",
    "\n",
    "# prepare embedding matrix\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "# index 0 is reserved for padding word_index to be masked by the embedding layer\n",
    "for word, i in filtered_word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL5xf0ehChWB"
   },
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_input_values = list(train_img_feats.values())\n",
    "n_img_features = np.array(image_input_values).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, Dense, Embedding, LSTM, Input, add\n",
    "from keras import backend as K\n",
    "# Merge model\n",
    "def define_merge_model(n_img_features, vocab_size, max_length, embedding_dim, embedding_matrix):\n",
    "    K.clear_session()\n",
    "    # feature extractor model\n",
    "    image_input = layers.Input(shape = (n_img_features, ))\n",
    "    fe1 = Dropout(0.5)(image_input)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    caption_input = layers.Input(shape = (max_length, ))\n",
    "    embedding_layer = layers.Embedding(vocab_size, embedding_dim,\n",
    "                                   weights = [embedding_matrix],\n",
    "                                   input_length = max_length,\n",
    "                                   mask_zero = False,\n",
    "                                   trainable = False)(caption_input)\n",
    "    se2 = Dropout(0.5)(embedding_layer)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # RNN Decoder\n",
    "# decoder = Sequential([\n",
    "#     layers.Input(shape=(None,)),\n",
    "#     layers.Embedding(vocab_size, embedding_dim, mask_zero=True),\n",
    "#     layers.LSTM(256, return_sequences=True),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.LSTM(256, return_sequences=True),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.LSTM(256, return_sequences=True),\n",
    "#     layers.Dropout(0.5),\n",
    "#     layers.TimeDistributed(layers.Dense(256, activation='relu')),\n",
    "#     layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))\n",
    "# ])\n",
    "\n",
    "# # Combine Encoder and Decoder\n",
    "# image_input  = layers.Input(shape = (image_input.shape[1], ))\n",
    "# fe1 = layers.Dropout(0.2)(image_input )\n",
    "# features  = layers.Dense(256, activation = 'relu')(fe1)\n",
    "# caption_input = layers.Input(shape=(max_length,))\n",
    "\n",
    "# # features = layers.Dropout(0.5)(features)\n",
    "# sequence = decoder(caption_input)\n",
    "# sequence = layers.Dropout(0.5)(sequence)\n",
    "# context = layers.Attention()([features, sequence])\n",
    "# outputs = layers.concatenate([context, sequence])\n",
    "# outputs = layers.TimeDistributed(layers.Dense(256, activation='relu'))(outputs)\n",
    "# outputs = layers.TimeDistributed(layers.Dense(vocab_size, activation='softmax'))(outputs)\n",
    "\n",
    "# model3 = Model(inputs=[image_input, caption_input], outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPygHAUZDIyn"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_merge = define_merge_model(n_img_features, vocab_size, max_length, embedding_dim, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "num_batches = len(train_df) // batch_size\n",
    "num_val_batches = len(test_df) // batch_size\n",
    "\n",
    "hist = model_merge.fit(train_generator,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=num_val_batches,\n",
    "          epochs = 25,\n",
    "          steps_per_epoch=num_batches,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inSz8Ua2TM6P",
    "tags": []
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('model_histpry.pkl', 'wb') as f:\n",
    "    pickle.dump(hist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('model_histpry.pkl', 'rb') as f:\n",
    "    hist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_YvQNHyTDhY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some custom metadata for the model\n",
    "metadata = {\n",
    "  'name': 'image_caption_generator',\n",
    "  'description': 'A model that generates captions for images using InceptionV3 and have all the stop words, and with no lemmatization',\n",
    "  'parameters': {\n",
    "    'vocab_size': 8000,\n",
    "    'embedding_dim': 200,\n",
    "    'lstm_units': 256,\n",
    "    'beam_size': 5\n",
    "  },\n",
    "  'performance': {\n",
    "    'loss': 1.6700,\n",
    "    'accuracy': 'Nan',\n",
    "    'bleu_score': 'NaN'\n",
    "  }\n",
    "}\n",
    "\n",
    "# save the model with the metadata\n",
    "model.save('capGen_model_v1.5.2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTQ_ucN_bVVo"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('capGen_model_v1.5.h5', custom_objects={'sparse_loss_function': sparse_loss_function})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_old = tf.keras.models.load_model('caption_generator_inceptionV3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQnHtzf5tnjL"
   },
   "source": [
    "# BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7MOHYaIRmLK"
   },
   "outputs": [],
   "source": [
    "def predict_caption_eval(image_vec):\n",
    "    image_vec = image_vec.reshape(1, -1)\n",
    "    # initialize the caption with the start token\n",
    "    caption = [tokenizer.word_index['startseq']]\n",
    "    # loop until the end token or the maximum length is reached\n",
    "\n",
    "    for i in range(max_length):\n",
    "        padded_caption = pad_sequences([caption], maxlen=max_length, padding='post')\n",
    "        # predict the next word using the model\n",
    "        prediction = model.predict([image_vec, padded_caption], verbose=0)\n",
    "        # get the word with the highest probability\n",
    "        word_index = np.argmax(prediction)\n",
    "        # append the word to the caption\n",
    "        caption.append(word_index)\n",
    "        # break if the end token is reached\n",
    "        if word_index == tokenizer.word_index['endseq']:\n",
    "            break\n",
    "    # convert the caption indices to words\n",
    "    caption_words = [tokenizer.index_word[i] for i in caption]\n",
    "    # join the words to form a sentence\n",
    "    caption_sentence = ' '.join(caption_words[1:-1])\n",
    "    return caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51l6uG3gQFwm",
    "outputId": "b48aa607-f27d-49b6-fa16-abacb8dc5c26"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "index_word = tokenizer.index_word\n",
    "\n",
    "nkeep = 5\n",
    "pred_good, pred_bad, bleus = [], [], [] \n",
    "count = 0 \n",
    "# fname: filenames from the dataframe\n",
    "# dt_test: tokenized text from tokenizer.texts_to_sequences()\n",
    "# di_test: image feature arrays\n",
    "test_img_feats_array = np.array(list(test_img_feats.values()))\n",
    "for jpgfnm, image_feature, tokenized_text in zip(test_df.filename.to_list(), test_img_feats_array, test_seqs):\n",
    "    count += 1\n",
    "    if count % 200 == 0:\n",
    "        print(\"  {:4.2f}% is done..\".format(100*count/float(len(test_df)/5)))\n",
    "\n",
    "    caption_true = [ index_word[i] for i in tokenized_text ]     \n",
    "    caption_true = caption_true[1:-1] ## remove startseq, and endseq\n",
    "    ## captions\n",
    "    caption = predict_caption_eval(image_feature)\n",
    "    caption = caption.split()\n",
    "\n",
    "    bleu = sentence_bleu([caption_true],caption)\n",
    "    bleus.append(bleu)\n",
    "    if bleu > 0.7 and len(pred_good) < nkeep:\n",
    "        pred_good.append((bleu,jpgfnm,caption_true,caption))\n",
    "    elif bleu < 0.3 and len(pred_bad) < nkeep:\n",
    "        pred_bad.append((bleu,jpgfnm,caption_true,caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O92EHaaMt5nL"
   },
   "source": [
    "# Predicting a caption for new image\n",
    "## greedy search and beam serach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.caption.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_file_dir = dir_Flickr_jpg + '/' + test_df.filename.iloc[0]\n",
    "true_caption = test_df.caption.iloc[0]\n",
    "# true_caption = [ tokenizer.index_word[i] for i in tokenizer.texts_to_sequences([true_caption])[0] ] \n",
    "print(f\"filename: {test_file_dir}\\ncaption {true_caption[1:-1]}\")\n",
    "test_img_feats[test_df.filename.iloc[0]]\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load an image\n",
    "img = Image.open(test_file_dir)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YvZZl5vMqaW",
    "outputId": "c2dc5aaf-b6ba-496f-e97f-4863a2a00757"
   },
   "outputs": [],
   "source": [
    "def predict_greedy_search(image_path, model):\n",
    "    # encode the image\n",
    "    image_vec = encode(image_path)\n",
    "    # reshape the vector, because our batch only includes 1 file prediction\n",
    "    image_vec = image_vec.reshape(1, -1)\n",
    "    # initialize the caption with the start token\n",
    "    caption = [tokenizer.word_index['startseq']]\n",
    "    # loop until the end token or the maximum length is reached\n",
    "\n",
    "    for i in range(max_length):\n",
    "        padded_caption = pad_sequences([caption], maxlen=22, padding='post')\n",
    "        # predict the next word using the model\n",
    "        prediction = model.predict([image_vec, padded_caption], verbose=0)\n",
    "        # get the word with the highest probability\n",
    "        word_index = np.argmax(prediction)\n",
    "        # append the word to the caption\n",
    "        caption.append(word_index)\n",
    "        # break if the end token is reached\n",
    "        if word_index == tokenizer.word_index['endseq']:\n",
    "            break\n",
    "    # convert the caption indices to words\n",
    "    caption_words = [tokenizer.index_word[i] for i in caption]\n",
    "    # join the words to form a sentence\n",
    "    caption_sentence = ' '.join(caption_words[1:-1])\n",
    "    return caption_sentence\n",
    "\n",
    "test_file_dir = dir_Flickr_jpg + '/' + test_df.filename.iloc[0]\n",
    "generated_caption = predict_greedy_search(test_file_dir, model_merge)\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r8I_m6Aneu4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_beam_search(image_path, beam_width, tokenizer, model):\n",
    "    # encode the image\n",
    "    image_vec = encode(image_path)\n",
    "    image_vec = image_vec.reshape(1, -1)\n",
    "    # initialize the caption with the start token\n",
    "    caption = tokenizer.texts_to_sequences([\"startseq\"])[0]\n",
    "    # initialize beam search\n",
    "    beam = [(caption, 0)]\n",
    "    \n",
    "    # loop until the end token or the maximum length is reached\n",
    "    for i in range(max_length):\n",
    "        # generate new candidates\n",
    "        candidates = []\n",
    "        for j in range(len(beam)):\n",
    "            seq, score = beam[j]\n",
    "            # check if the sequence ends with endseq\n",
    "            if seq[-1] == tokenizer.word_index[\"endseq\"]:\n",
    "                candidates.append((seq, score))\n",
    "                continue\n",
    "            # predict the next word using the model\n",
    "            padded_caption = pad_sequences([seq], maxlen=max_length, padding='post')\n",
    "            prediction = model.predict([image_vec, padded_caption], verbose=0)[0]\n",
    "            # get the top k words with the highest probability\n",
    "            top_k = prediction.argsort()[-beam_width:][::-1]\n",
    "            # add new candidates to the list\n",
    "            for w in top_k:\n",
    "                new_seq = seq + [w]\n",
    "                new_score = score + np.log(prediction[w])\n",
    "                candidates.append((new_seq, new_score))\n",
    "        # select top k candidates\n",
    "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "    # select the best candidate\n",
    "    seq, score = beam[0]\n",
    "    # convert the caption indices to words\n",
    "    caption_words = tokenizer.sequences_to_texts([seq])[0].split()\n",
    "    # join the words to form a sentence\n",
    "    caption_sentence = ' '.join(caption_words[1:-1])\n",
    "    return caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d0r5UjhV8Q-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_file_dir = dir_Flickr_jpg + '/' + test_df.filename.iloc[0]\n",
    "\n",
    "generated_caption = predict_beam_search(test_file_dir, 10, tokenizer, model=model_old)\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cffdeMRmHEb"
   },
   "source": [
    "# Model 2, more complicated\n",
    "* You can use a bidirectional LSTM instead of a single LSTM for the caption encoder. This way, you can capture the context from both directions of the caption sequence, and generate more coherent captions.\n",
    "\n",
    "* You can use an attention mechanism to allow the decoder to focus on different parts of the image and the caption encoder outputs at each time step. This way, you can generate more relevant and informative captions that align with the image content.\n",
    "\n",
    "* You can use a scheduled sampling technique to train the decoder with a mix of ground truth and predicted words. This way, you can reduce the exposure bias and improve the generalization ability of the decoder.\n",
    "\n",
    "* You can use a beam search instead of a greedy search for generating captions. This way, you can explore more possible captions and choose the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93vaiXH4mJx5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "# define the model\n",
    "ip1 = layers.Input(shape = (2048, ))\n",
    "fe1 = layers.Dropout(0.2)(ip1)\n",
    "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
    "ip2 = layers.Input(shape = (max_length, ))\n",
    "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
    "se2 = layers.Dropout(0.2)(se1)\n",
    "se3 = Bidirectional(layers.LSTM(256, return_sequences=True))(se2) # use bidirectional LSTM\n",
    "decoder1 = layers.add([fe2, se3[:, -1]]) # use last hidden state of bidirectional LSTM\n",
    "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
    "attn_layer = Attention() # use attention layer\n",
    "context_vector, attention_weights = attn_layer([decoder2, se3]) # get context vector and attention weights\n",
    "decoder3 = layers.Dense(256, activation='relu')(context_vector) # use context vector for final dense layer\n",
    "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder3)\n",
    "model_v2 = Model(inputs = [ip1, ip2], outputs = outputs)\n",
    "\n",
    "model_v2.layers[3].set_weights([embedding_matrix])\n",
    "model_v2.layers[3].trainable = False\n",
    "model_v2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "num_batches = len(df_txt0) // batch_size\n",
    "model_v2.fit(train_generator, epochs = 50, steps_per_epoch=num_batches, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ6uH3DimysF"
   },
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgdmGhyLmy8M"
   },
   "outputs": [],
   "source": [
    "def beam_search(image_path, beam_size):\n",
    "  # encode the image\n",
    "  image_vec = encode(image_path)\n",
    "  # add another dimension to match the model input\n",
    "  image_vec = np.expand_dims(image_vec, axis=0)\n",
    "  # initialize the candidates with the start token\n",
    "  candidates = [[tokenizer.word_index['<start>']]]\n",
    "  # initialize the probabilities with 1\n",
    "  probabilities = [1]\n",
    "  # loop until the maximum length is reached\n",
    "  for i in range(max_length):\n",
    "    # initialize a list to store the next candidates\n",
    "    next_candidates = []\n",
    "    # initialize a list to store the next probabilities\n",
    "    next_probabilities = []\n",
    "    # loop over the current candidates\n",
    "    for j in range(len(candidates)):\n",
    "      # get the current candidate\n",
    "      candidate = candidates[j]\n",
    "      # pad the candidate sequence\n",
    "      padded_candidate = pad_sequences([candidate], maxlen=max_length, padding='post')\n",
    "      # predict the next word using the model\n",
    "      prediction = model.predict([image_vec, padded_candidate], verbose=0)\n",
    "      # get the top beam_size words and their probabilities\n",
    "      top_words = np.argsort(prediction[0])[-beam_size:]\n",
    "      top_probs = prediction[0][top_words]\n",
    "      # loop over the top words and their probabilities\n",
    "      for k in range(beam_size):\n",
    "        # get the word and its probability\n",
    "        word = top_words[k]\n",
    "        prob = top_probs[k]\n",
    "        # append the word to the candidate and multiply the probability\n",
    "        next_candidate = candidate + [word]\n",
    "        next_prob = probabilities[j] * prob\n",
    "        # append the next candidate and probability to the lists\n",
    "        next_candidates.append(next_candidate)\n",
    "        next_probabilities.append(next_prob)\n",
    "    # sort the next candidates and probabilities by descending order of probability\n",
    "    sorted_indices = np.argsort(next_probabilities)[::-1]\n",
    "    sorted_candidates = [next_candidates[i] for i in sorted_indices]\n",
    "    sorted_probabilities = [next_probabilities[i] for i in sorted_indices]\n",
    "    # select the top beam_size candidates and probabilities for the next iteration\n",
    "    candidates = sorted_candidates[:beam_size]\n",
    "    probabilities = sorted_probabilities[:beam_size]\n",
    "    # check if any candidate has reached the end token\n",
    "    end_index = tokenizer.word_index['<end>']\n",
    "    if any(candidate[-1] == end_index for candidate in candidates):\n",
    "      break\n",
    "  # return the candidate with the highest probability\n",
    "  best_candidate = candidates[0]\n",
    "  # convert the candidate indices to words\n",
    "  caption_words = [tokenizer.index_word[i] for i in best_candidate]\n",
    "  # join the words to form a sentence\n",
    "  caption_sentence = ' '.join(caption_words[1:-1])\n",
    "  return caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSA86kO8z39g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(df, image_data, batch_size):\n",
    "    # create a dataset from the data frame\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df['filename'], df['caption']))\n",
    "    # map the filename to the image data\n",
    "    ds = ds.map(lambda x, y: (image_data[x], y))\n",
    "    # apply the TextVectorization layer as a transformation\n",
    "    ds = ds.map(lambda x, y: (x, vectorizer([y])))\n",
    "    # unbatch the dataset to get individual elements\n",
    "    ds = ds.unbatch()\n",
    "    # create input and output sequences\n",
    "    ds = ds.map(lambda x, y: (x, y[:-1], y[1:]))\n",
    "    # pad and one-hot encode the sequences if needed\n",
    "    # ds = ds.map(lambda x, y, z: (x, pad_sequences([y], maxlen=max_length)[0], to_categorical([z], num_classes=vocab_size)[0]))\n",
    "    # batch the dataset\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = data_generator(df_txt, image_data, batch_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
