{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Delaunay-I/image_cap_generator/blob/main/caption_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/colab\\ files\n",
        "!pwd"
      ],
      "metadata": {
        "id": "MiYyEUpAzqtf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c4ed26-869a-4d32-c954-81a38837f3bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab files\n",
            "/content/drive/MyDrive/colab files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUtHdFGhbjPf",
        "outputId": "21991ae9-df5c-4f69-eb8f-c8c9c6d7f4b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6Xkro5uPCo_",
        "outputId": "c98b4a29-debd-40a7-b7c6-7353f6d34df7",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "keras version 2.12.0\n",
            "tensorflow version 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import sys, time, os, warnings \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from collections import Counter \n",
        "\n",
        "print(\"python {}\".format(sys.version))\n",
        "print(\"keras version {}\".format(keras.__version__)); del keras\n",
        "print(\"tensorflow version {}\".format(tf.__version__))\n",
        "\n",
        "def set_seed(sd=123):\n",
        "    from numpy.random import seed\n",
        "    from tensorflow import set_random_seed\n",
        "    import random as rn\n",
        "    ## numpy random seed\n",
        "    seed(sd)\n",
        "    ## core python's random number \n",
        "    rn.seed(sd)\n",
        "    ## tensor flow's random number\n",
        "    set_random_seed(sd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu7zkvTxPT-9",
        "outputId": "7122871c-72c9-4c6e-b686-1771772ed67b",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Skipping, found downloaded files in \"./flickr8k\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/adityajn105/flickr8k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNw0Wid3Vlpu",
        "outputId": "f2ff4258-ab34-4ffa-9fdd-57d325601609",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of jpg flies in Flicker8k: 8091\n"
          ]
        }
      ],
      "source": [
        "## The location of the Flickr8K_ photos\n",
        "dir_Flickr_jpg = \"/content/drive/MyDrive/colab files/flickr8k/Images\"\n",
        "## The location of the caption file\n",
        "dir_Flickr_text = \"/content/drive/MyDrive/colab files/flickr8k/captions.txt\"\n",
        "\n",
        "jpgs = os.listdir(dir_Flickr_jpg)\n",
        "print(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45SpOt_GXL-u"
      },
      "source": [
        "## Preliminary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YVHrZfSYxmO",
        "outputId": "19387a41-4663-43cf-e52c-96658571783f",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique file names : 8091\n",
            "The distribution of the number of captions for each image:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({5: 8091})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_txt = pd.read_csv(dir_Flickr_text, skiprows=1, names=[\"filename\", \"caption\"])\n",
        "df_txt['caption'] = df_txt['caption'].str.lower()\n",
        "\n",
        "df_txt['index'] = df_txt.groupby(\"filename\").cumcount()\n",
        "\n",
        "uni_filenames = np.unique(df_txt.filename.values)\n",
        "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
        "print(\"The distribution of the number of captions for each image:\")\n",
        "Counter(Counter(df_txt.filename.values).values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEq8DiKoR_Y"
      },
      "source": [
        "# Data prepration\n",
        "prepare text and image separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HMihehhY6Ps7",
        "outputId": "37f1dac2-eb1a-42cc-88ad-c8777afcb93f",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    filename  \\\n",
              "0  1000268201_693b08cb0e.jpg   \n",
              "1  1000268201_693b08cb0e.jpg   \n",
              "2  1000268201_693b08cb0e.jpg   \n",
              "3  1000268201_693b08cb0e.jpg   \n",
              "4  1000268201_693b08cb0e.jpg   \n",
              "\n",
              "                                             caption  index  \n",
              "0  startseq a child in a pink dress is climbing u...      0  \n",
              "1  startseq a girl going into a wooden building ....      1  \n",
              "2  startseq a little girl climbing into a wooden ...      2  \n",
              "3  startseq a little girl climbing the stairs to ...      3  \n",
              "4  startseq a little girl in a pink dress going i...      4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8ec7517-2aaa-4630-afbc-d7ea85562e7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>caption</th>\n",
              "      <th>index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a child in a pink dress is climbing u...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a girl going into a wooden building ....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a little girl climbing into a wooden ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a little girl climbing the stairs to ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a little girl in a pink dress going i...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8ec7517-2aaa-4630-afbc-d7ea85562e7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8ec7517-2aaa-4630-afbc-d7ea85562e7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8ec7517-2aaa-4630-afbc-d7ea85562e7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from copy import copy\n",
        "def add_start_end_seq_token(captions):\n",
        "    caps = []\n",
        "    for txt in captions:\n",
        "        txt = 'startseq ' + txt + ' endseq'\n",
        "        caps.append(txt)\n",
        "    return(caps)\n",
        "\n",
        "df_txt[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
        "df_txt.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfZe4SJDGlo"
      },
      "source": [
        "# split the dataset int train and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MxqCDgSDDFKw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split your df_txt into train and test sets\n",
        "train_df, test_df = train_test_split(df_txt, test_size=0.2, shuffle=False)\n",
        "# dropping the imagefile that with shared captions in the two splits\n",
        "# this would also avoid data leakage\n",
        "test_df = test_df.iloc[1:]\n",
        "train_df =  train_df[:-4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3L06Z6A8aSl"
      },
      "source": [
        "# Image prepration\n",
        "## create features for image using InceptionV3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GgffeHrI8d49",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f18f281-410a-4ef9-86b1-e9f301e10703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96112376/96112376 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "image_model = Model(inputs = base_model.input, outputs=base_model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O2ZHDosMOXWn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "import glob\n",
        "import pickle\n",
        "\n",
        "train_path = dir_Flickr_jpg\n",
        "path_all_images = glob.glob(train_path + '/*jpg')\n",
        "\n",
        "train_img = []  # list of all images in training set\n",
        "test_img = []\n",
        "for im in path_all_images:\n",
        "    file_name = os.path.basename(os.path.normpath(im))\n",
        "    # include images that only exist in the target directory\n",
        "    # can split the dataset this way\n",
        "    if(file_name in train_df.filename.to_list()):\n",
        "        train_img.append(im)\n",
        "    elif (file_name in test_df.filename.to_list()):\n",
        "        test_img.append(im)\n",
        "\n",
        "def preprocess(image_path):\n",
        "    # inception v3 excepts img in 299 * 299 * 3\n",
        "    image = load_img(image_path, target_size=(299, 299))\n",
        "    # convert the image pixels to a numpy array\n",
        "    x = img_to_array(image)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis = 0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "def encode(image_path):\n",
        "    image = preprocess(image_path)\n",
        "    vec = image_model.predict(image, verbose=0)\n",
        "    vec_flattened = vec.flatten()\n",
        "    return vec_flattened\n",
        "\n",
        "\n",
        "train_img_feats = {}\n",
        "test_img_feats = {}\n",
        "\n",
        "if not (os.path.exists('train_encoder.pkl') and os.path.exists('test_encoder.pkl')):\n",
        "    for image in train_img:\n",
        "        file_name = os.path.basename(os.path.normpath(image))\n",
        "        train_img_feats[file_name] = encode(image)\n",
        "    for image in test_img:\n",
        "        file_name = os.path.basename(os.path.normpath(image))\n",
        "        test_img_feats[file_name] = encode(image)\n",
        "    # Save the image features\n",
        "    with open('train_encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(train_img_feats, f)\n",
        "    with open('test_encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(test_img_feats, f)\n",
        "else:\n",
        "    # Load previously encoded image data\n",
        "    with open('train_encoder.pkl', 'rb') as f:\n",
        "        train_img_feats = pickle.load(f)\n",
        "    with open('test_encoder.pkl', 'rb') as f:\n",
        "        test_img_feats = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAoKpyScvQXD"
      },
      "source": [
        "# Tokenize the captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O06giiUErwmI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vocab_size = 8000\n",
        "# output_sequence_length = 20 # adjust this according to your max_length\n",
        "\n",
        "# create the TextVectorization layer\n",
        "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode=\"int\")\n",
        "\n",
        "# adapt the layer to the captions_list\n",
        "vectorizer.adapt(train_df.caption.to_list())\n",
        "\n",
        "train_seqs = vectorizer(train_df.caption.to_list())\n",
        "test_seqs = vectorizer(test_df.caption.to_list())\n",
        "\n",
        "vectorizer_word_index = {}\n",
        "vectorizer_index_word = {}\n",
        "for index, word in enumerate(vectorizer.get_vocabulary()):\n",
        "    if index == 0:\n",
        "        continue\n",
        "    vectorizer_word_index[word] = index\n",
        "    vectorizer_index_word[index] = word\n",
        "\n",
        "# get the max length of sequences from the vectorizer\n",
        "max_length = train_seqs.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "vocab_size = 8000\n",
        "# Tokenizer does not limit the number of words\n",
        "# it still finds all the words in the word_index\n",
        "# But it will only use the num_words given to encode the text in texts_to_sequences or sequences_to_texts methods\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df.caption.to_list())\n",
        "\n",
        "# get the word index\n",
        "train_seqs = tokenizer.texts_to_sequences(train_df.caption.to_list())\n",
        "test_seq = tokenizer.texts_to_sequences(test_df.caption.to_list())\n",
        "\n",
        "# calculate the maximum caption length\n",
        "max_length = max(len(seq) for seq in train_seqs)"
      ],
      "metadata": {
        "id": "bB7HT-frcF-g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define a data generator function\n",
        "def data_generator(dataframe, img_feats, tokenizer, max_len, batch_size):\n",
        "  # Initialize variables\n",
        "  x1, x2, y = [], [], []\n",
        "  n = 0\n",
        "  # Loop over the dataframe rows\n",
        "  for index, row in dataframe.iterrows():\n",
        "    # Get the image filename and caption\n",
        "    img_name = row['filename']\n",
        "    caption = row['caption']\n",
        "    # Get the image features from the dictionary\n",
        "    img_feat = img_feats[img_name]\n",
        "    # Convert the caption to a sequence of integers\n",
        "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "    # Split the sequence into multiple input-output pairs\n",
        "    for i in range(1, len(seq)):\n",
        "      # Get the input and output sequence\n",
        "      in_seq = seq[:i]\n",
        "      out_seq = seq[i]\n",
        "      # Pad the input sequence to the maximum length\n",
        "      in_seq = pad_sequences([in_seq], maxlen=max_len)[0]\n",
        "      # Encode the output sequence as one-hot vector\n",
        "      out_seq = to_categorical([out_seq], num_classes=len(tokenizer.word_index)+1)[0]\n",
        "      # Append the image features, input sequence and output sequence to the lists\n",
        "      x1.append(img_feat)\n",
        "      x2.append(in_seq)\n",
        "      y.append(out_seq)\n",
        "      # Increment the counter\n",
        "      n += 1\n",
        "      # Check if the batch size is reached\n",
        "      if n == batch_size:\n",
        "        # Yield the batch as numpy arrays\n",
        "        yield [np.array(x1), np.array(x2)], np.array(y)\n",
        "        # Reset the lists and counter\n",
        "        x1, x2, y = [], [], []\n",
        "        n = 0\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 64\n",
        "# Create a train generator and a valid generator using the data generator function\n",
        "train_generator = data_generator(train_df, train_img_feats, tokenizer, max_length, batch_size)\n",
        "valid_generator = data_generator(test_df, test_img_feats, tokenizer, max_length, batch_size)"
      ],
      "metadata": {
        "id": "PsU5RFQUcusP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_LjvGtNAUUw"
      },
      "source": [
        "# Downloading GloVe to using its vector embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fddgWxT36kCN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Set the URL for the GloVe embeddings\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "\n",
        "# Set the path where the embeddings will be stored\n",
        "embeddings_dir = 'embeddings/glove'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(embeddings_dir):\n",
        "    os.makedirs(embeddings_dir)\n",
        "\n",
        "# Set the file name for the embeddings archive\n",
        "embeddings_zip = os.path.join(embeddings_dir, 'glove.6B.zip')\n",
        "\n",
        "# Download the embeddings archive if it doesn't exist\n",
        "if not os.path.exists(embeddings_zip):\n",
        "    print(f'Downloading GloVe embeddings from {url}...')\n",
        "    urllib.request.urlretrieve(url, embeddings_zip)\n",
        "    print('Done!')\n",
        "\n",
        "# Extract the embeddings if they haven't been extracted yet\n",
        "if not os.path.exists(os.path.join(embeddings_dir, 'glove.6B.100d.txt')):\n",
        "    print('Extracting GloVe embeddings...')\n",
        "    with zipfile.ZipFile(embeddings_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(embeddings_dir)\n",
        "    print('Done!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rGfT12woASi-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# load GloVe embeddings\n",
        "embeddings_index = {}\n",
        "glove_path = \"./embeddings/glove/glove.6B.200d.txt\"\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# filter out the words that are not in the top num_words-1 most frequent words\n",
        "filtered_word_index = {word: i for word, i in tokenizer.word_index.items() if i < vocab_size}\n",
        "\n",
        "# prepare embedding matrix\n",
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in filtered_word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL5xf0ehChWB"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V_3NT4N7CjxV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# define the model\n",
        "ip1 = layers.Input(shape = (2048, ))\n",
        "fe1 = layers.Dropout(0.2)(ip1)\n",
        "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
        "ip2 = layers.Input(shape = (max_length, ))\n",
        "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
        "se2 = layers.Dropout(0.2)(se1)\n",
        "se3 = layers.LSTM(256)(se2)\n",
        "decoder1 = layers.add([fe2, se3])\n",
        "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
        "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder2)\n",
        "model = Model(inputs = [ip1, ip2], outputs = outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPygHAUZDIyn"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"test_df: {test_df.shape}\")\n",
        "print(f\"train_df: {train_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_DFKuhL4KBY",
        "outputId": "82cc88e3-ff90-4c94-cab3-427a0dfb109d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_df: (8090, 3)\n",
            "train_df: (32360, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df) // batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nia_P8St3-b6",
        "outputId": "d5d83230-5fd5-4249-907b-f9f5d58e2bca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "505"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df)  // batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7CmuKOZ36Ax",
        "outputId": "212e21b4-aa25-44a3-8120-9dda3a62eacf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ESmmmVAjDLla",
        "outputId": "bfc670a4-cd0a-4ded-9f3d-3315c5e9def6",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a3fe5529398b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_val_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model.fit(train_generator,\n\u001b[0m\u001b[1;32m      9\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_val_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'categorical_crossentropy/softmax_cross_entropy_with_logits' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-26-a3fe5529398b>\", line 8, in <cell line: 8>\n      model.fit(train_generator,\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n      return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/backend.py\", line 5565, in categorical_crossentropy\n      return tf.nn.softmax_cross_entropy_with_logits(\nNode: 'categorical_crossentropy/softmax_cross_entropy_with_logits'\nlogits and labels must be broadcastable: logits_size=[64,8000] labels_size=[64,7761]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_train_function_19403]"
          ]
        }
      ],
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "num_batches = len(train_df) // batch_size\n",
        "num_val_batches = len(test_df) // batch_size\n",
        "\n",
        "model.fit(train_generator,\n",
        "          validation_data=valid_generator,\n",
        "          validation_steps=num_val_batches,\n",
        "          epochs = 50,\n",
        "          steps_per_epoch=num_batches,\n",
        "          verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inSz8Ua2TM6P"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_YvQNHyTDhY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define some custom metadata for the model\n",
        "metadata = {\n",
        "  'name': 'image_caption_generator',\n",
        "  'description': 'A model that generates captions for images using InceptionV3 and have all the stop words, and with no lemmatization',\n",
        "  'parameters': {\n",
        "    'vocab_size': 8000,\n",
        "    'embedding_dim': 200,\n",
        "    'lstm_units': 256,\n",
        "    'beam_size': 5\n",
        "  },\n",
        "  'performance': {\n",
        "    'loss': 1.6700,\n",
        "    'accuracy': 'Nan',\n",
        "    'bleu_score': 'NaN'\n",
        "  }\n",
        "}\n",
        "\n",
        "# save the model with the metadata\n",
        "model.save('capGen_model_v2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D02QybmdJ3F2"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('img_cap_model_v2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4C8c76HDNes",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the new image\n",
        "img = encode('./test_img.jpg')\n",
        "img = img.reshape((1, 2048))\n",
        "\n",
        "# Generate the caption\n",
        "caption = [tokenizer.word_index[\"startseq\"]]\n",
        "\n",
        "for i in range(max_length):\n",
        "    seq = pad_sequences([caption], maxlen=max_length)\n",
        "    pred = model.predict([img, seq])\n",
        "    pred_word_index = np.argmax(pred[0][i])\n",
        "    if pred_word_index in tokenizer.index_word:\n",
        "        pred_word = tokenizer.index_word[pred_word_index]\n",
        "        caption.append(pred_word_index)\n",
        "        if pred_word == 'endseq':\n",
        "            break\n",
        "\n",
        "# Convert the caption back to text\n",
        "caption = tokenizer.sequences_to_texts([caption])[0]\n",
        "\n",
        "print(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zFWdcKA8cCqz"
      },
      "outputs": [],
      "source": [
        "model_old = tf.keras.models.load_model(\"./caption_generator_inceptionV3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-BydfZoYSi7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the new image\n",
        "img = encode(\"test_img.jpg\")\n",
        "img = img.reshape((1, 2048))\n",
        "\n",
        "caption = [vectorizer([\"startseq\"]).numpy()[0][0]]\n",
        "\n",
        "for i in range(max_length):\n",
        "    padded_caption = pad_sequences([caption], maxlen=22, padding='post')\n",
        "    prediction = model.predict([img, padded_caption], verbose=0)\n",
        "    word_index = np.argmax(prediction)\n",
        "    caption.append(word_index)\n",
        "    if word_index == vectorizer_word_index[\"endseq\"]:\n",
        "        break\n",
        "\n",
        "\n",
        "caption_words = [vectorizer_index_word[i] for i in caption]\n",
        "# join the words to form a sentence\n",
        "caption_sentence = ' '.join(caption_words[1:-1])\n",
        "caption_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TlqWDI6ecCq1"
      },
      "outputs": [],
      "source": [
        "def beam_search_caption(image_path, beam_width):\n",
        "    # encode the image\n",
        "    image_vec = encode(image_path)\n",
        "    image_vec = image_vec.reshape(1, -1)\n",
        "    # initialize the caption with the start token\n",
        "    caption = [vectorizer([\"startseq\"]).numpy()[0][0]]\n",
        "    # initialize beam search\n",
        "    beam = [(caption, 0)]\n",
        "    \n",
        "    # loop until the end token or the maximum length is reached\n",
        "    for i in range(max_length):\n",
        "        # generate new candidates\n",
        "        candidates = []\n",
        "        for j in range(len(beam)):\n",
        "            seq, score = beam[j]\n",
        "            # check if the sequence ends with endseq\n",
        "            if seq[-1] == vectorizer_word_index[\"endseq\"]:\n",
        "                candidates.append((seq, score))\n",
        "                continue\n",
        "            # predict the next word using the model\n",
        "            padded_caption = pad_sequences([seq], maxlen=max_length, padding='post')\n",
        "            prediction = model.predict([image_vec, padded_caption], verbose=0)[0]\n",
        "            # get the top k words with the highest probability\n",
        "            top_k = prediction.argsort()[-beam_width:][::-1]\n",
        "            # add new candidates to the list\n",
        "            for w in top_k:\n",
        "                new_seq = seq + [w]\n",
        "                new_score = score + np.log(prediction[w])\n",
        "                candidates.append((new_seq, new_score))\n",
        "        # select top k candidates\n",
        "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "        \n",
        "    # select the best candidate\n",
        "    seq, score = beam[0]\n",
        "    # convert the caption indices to words\n",
        "    caption_words = [vectorizer_index_word[i] for i in seq]\n",
        "    # join the words to form a sentence\n",
        "    caption_sentence = ' '.join(caption_words[1:-1])\n",
        "    return caption_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d0r5UjhV8Q-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "generated_caption = beam_search_caption(\"test_img.jpg\", 10)\n",
        "print(generated_caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7owjVv1BmGZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cffdeMRmHEb"
      },
      "source": [
        "# Model 2, more complicated\n",
        "* You can use a bidirectional LSTM instead of a single LSTM for the caption encoder. This way, you can capture the context from both directions of the caption sequence, and generate more coherent captions.\n",
        "\n",
        "* You can use an attention mechanism to allow the decoder to focus on different parts of the image and the caption encoder outputs at each time step. This way, you can generate more relevant and informative captions that align with the image content.\n",
        "\n",
        "* You can use a scheduled sampling technique to train the decoder with a mix of ground truth and predicted words. This way, you can reduce the exposure bias and improve the generalization ability of the decoder.\n",
        "\n",
        "* You can use a beam search instead of a greedy search for generating captions. This way, you can explore more possible captions and choose the one with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93vaiXH4mJx5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Attention\n",
        "\n",
        "# define the model\n",
        "ip1 = layers.Input(shape = (2048,))\n",
        "fe1 = layers.Dropout(0.2)(ip1)\n",
        "fe2 = layers.Dense(512, activation='relu')(fe1)  # add this layer to match the dimension of se3[:, -1]\n",
        "ip2 = layers.Input(shape = (max_length,))\n",
        "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
        "se2 = layers.Dropout(0.2)(se1)\n",
        "se3 = Bidirectional(layers.LSTM(256, return_sequences=True))(se2)\n",
        "decoder1 = layers.add([fe2, se3[:, -1]])\n",
        "decoder2 = layers.Dense(256, activation='relu')(decoder1)\n",
        "attn_layer = Attention()\n",
        "context_vector, attention_weights = attn_layer([decoder2, se3])\n",
        "decoder3 = layers.Dense(256, activation='relu')(context_vector)\n",
        "outputs = layers.Dense(vocab_size, activation='softmax')(decoder3)\n",
        "model_v2 = Model(inputs=[ip1, ip2], outputs=outputs)\n",
        "\n",
        "model_v2.layers[3].set_weights([embedding_matrix])\n",
        "model_v2.layers[3].trainable = False\n",
        "model_v2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "num_batches = len(train_df) // batch_size\n",
        "model_v2.fit(train_generator, epochs = 50, steps_per_epoch=num_batches, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UXfPrCcmxsq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ6uH3DimysF"
      },
      "source": [
        "## Beam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgdmGhyLmy8M"
      },
      "outputs": [],
      "source": [
        "def beam_search(image_path, beam_size):\n",
        "  # encode the image\n",
        "  image_vec = encode(image_path)\n",
        "  # add another dimension to match the model input\n",
        "  image_vec = np.expand_dims(image_vec, axis=0)\n",
        "  # initialize the candidates with the start token\n",
        "  candidates = [[tokenizer.word_index['<start>']]]\n",
        "  # initialize the probabilities with 1\n",
        "  probabilities = [1]\n",
        "  # loop until the maximum length is reached\n",
        "  for i in range(max_length):\n",
        "    # initialize a list to store the next candidates\n",
        "    next_candidates = []\n",
        "    # initialize a list to store the next probabilities\n",
        "    next_probabilities = []\n",
        "    # loop over the current candidates\n",
        "    for j in range(len(candidates)):\n",
        "      # get the current candidate\n",
        "      candidate = candidates[j]\n",
        "      # pad the candidate sequence\n",
        "      padded_candidate = pad_sequences([candidate], maxlen=max_length, padding='post')\n",
        "      # predict the next word using the model\n",
        "      prediction = model.predict([image_vec, padded_candidate], verbose=0)\n",
        "      # get the top beam_size words and their probabilities\n",
        "      top_words = np.argsort(prediction[0])[-beam_size:]\n",
        "      top_probs = prediction[0][top_words]\n",
        "      # loop over the top words and their probabilities\n",
        "      for k in range(beam_size):\n",
        "        # get the word and its probability\n",
        "        word = top_words[k]\n",
        "        prob = top_probs[k]\n",
        "        # append the word to the candidate and multiply the probability\n",
        "        next_candidate = candidate + [word]\n",
        "        next_prob = probabilities[j] * prob\n",
        "        # append the next candidate and probability to the lists\n",
        "        next_candidates.append(next_candidate)\n",
        "        next_probabilities.append(next_prob)\n",
        "    # sort the next candidates and probabilities by descending order of probability\n",
        "    sorted_indices = np.argsort(next_probabilities)[::-1]\n",
        "    sorted_candidates = [next_candidates[i] for i in sorted_indices]\n",
        "    sorted_probabilities = [next_probabilities[i] for i in sorted_indices]\n",
        "    # select the top beam_size candidates and probabilities for the next iteration\n",
        "    candidates = sorted_candidates[:beam_size]\n",
        "    probabilities = sorted_probabilities[:beam_size]\n",
        "    # check if any candidate has reached the end token\n",
        "    end_index = tokenizer.word_index['<end>']\n",
        "    if any(candidate[-1] == end_index for candidate in candidates):\n",
        "      break\n",
        "  # return the candidate with the highest probability\n",
        "  best_candidate = candidates[0]\n",
        "  # convert the candidate indices to words\n",
        "  caption_words = [tokenizer.index_word[i] for i in best_candidate]\n",
        "  # join the words to form a sentence\n",
        "  caption_sentence = ' '.join(caption_words[1:-1])\n",
        "  return caption_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSA86kO8z39g"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "def data_generator(df, image_data, batch_size):\n",
        "    # create a dataset from the data frame\n",
        "    ds = tf.data.Dataset.from_tensor_slices((df['filename'], df['caption']))\n",
        "    # map the filename to the image data\n",
        "    ds = ds.map(lambda x, y: (image_data[x], y))\n",
        "    # apply the TextVectorization layer as a transformation\n",
        "    ds = ds.map(lambda x, y: (x, vectorizer([y])))\n",
        "    # unbatch the dataset to get individual elements\n",
        "    ds = ds.unbatch()\n",
        "    # create input and output sequences\n",
        "    ds = ds.map(lambda x, y: (x, y[:-1], y[1:]))\n",
        "    # pad and one-hot encode the sequences if needed\n",
        "    # ds = ds.map(lambda x, y, z: (x, pad_sequences([y], maxlen=max_length)[0], to_categorical([z], num_classes=vocab_size)[0]))\n",
        "    # batch the dataset\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds\n",
        "\n",
        "batch_size = 64\n",
        "train_generator = data_generator(df_txt, image_data, batch_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}