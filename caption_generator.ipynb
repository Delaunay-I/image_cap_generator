{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Delaunay-I/image_cap_generator/blob/main/caption_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/colab\\ files\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2iYRwSDnkOa",
        "outputId": "90172f6c-d9e7-4556-b8ee-889ef427ac3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab files\n",
            "/content/drive/MyDrive/colab files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6Xkro5uPCo_",
        "outputId": "966c6e67-0d19-4aad-a54c-13fe9f94db94",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "keras version 2.12.0\n",
            "tensorflow version 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import sys, time, os, warnings \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from collections import Counter \n",
        "\n",
        "print(\"python {}\".format(sys.version))\n",
        "print(\"keras version {}\".format(keras.__version__)); del keras\n",
        "print(\"tensorflow version {}\".format(tf.__version__))\n",
        "\n",
        "def set_seed(sd=123):\n",
        "    from numpy.random import seed\n",
        "    from tensorflow import set_random_seed\n",
        "    import random as rn\n",
        "    ## numpy random seed\n",
        "    seed(sd)\n",
        "    ## core python's random number \n",
        "    rn.seed(sd)\n",
        "    ## tensor flow's random number\n",
        "    set_random_seed(sd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu7zkvTxPT-9",
        "outputId": "fedd1293-24f4-4421-867f-e5702e42ef31",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.9/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from opendatasets) (8.1.3)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.9/dist-packages (from opendatasets) (1.5.13)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.26.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle->opendatasets) (3.4)\n",
            "Skipping, found downloaded files in \"./flickr8k\" (use force=True to force download)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/adityajn105/flickr8k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNw0Wid3Vlpu",
        "outputId": "0a8aa96b-3933-4e16-b591-1de4020f5aa3",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of jpg flies in Flicker8k: 8091\n"
          ]
        }
      ],
      "source": [
        "## The location of the Flickr8K_ photos\n",
        "dir_Flickr_jpg = \"/content/drive/MyDrive/colab files/flickr8k/Images\"\n",
        "## The location of the caption file\n",
        "dir_Flickr_text = \"/content/drive/MyDrive/colab files/flickr8k/captions.txt\"\n",
        "\n",
        "jpgs = os.listdir(dir_Flickr_jpg)\n",
        "print(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45SpOt_GXL-u"
      },
      "source": [
        "## Preliminary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YVHrZfSYxmO",
        "outputId": "8042ce8e-4c63-446a-f8ad-223306fa9499",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unique file names : 8091\n",
            "The distribution of the number of captions for each image:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({5: 8091})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_txt = pd.read_csv(dir_Flickr_text, skiprows=1, names=[\"filename\", \"caption\"])\n",
        "df_txt['caption'] = df_txt['caption'].str.lower()\n",
        "\n",
        "df_txt['index'] = df_txt.groupby(\"filename\").cumcount()\n",
        "\n",
        "uni_filenames = np.unique(df_txt.filename.values)\n",
        "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
        "print(\"The distribution of the number of captions for each image:\")\n",
        "Counter(Counter(df_txt.filename.values).values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEq8DiKoR_Y"
      },
      "source": [
        "# Data prepration\n",
        "prepare text and image separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HMihehhY6Ps7",
        "outputId": "2628b941-3abe-4053-b801-13c61aaec265",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    filename  \\\n",
              "0  1000268201_693b08cb0e.jpg   \n",
              "1  1000268201_693b08cb0e.jpg   \n",
              "2  1000268201_693b08cb0e.jpg   \n",
              "3  1000268201_693b08cb0e.jpg   \n",
              "4  1000268201_693b08cb0e.jpg   \n",
              "\n",
              "                                             caption  index  \n",
              "0  startseq a child in a pink dress is climbing u...      0  \n",
              "1  startseq a girl going into a wooden building ....      1  \n",
              "2  startseq a little girl climbing into a wooden ...      2  \n",
              "3  startseq a little girl climbing the stairs to ...      3  \n",
              "4  startseq a little girl in a pink dress going i...      4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c088c444-9dff-4751-a2ab-229210f384db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>caption</th>\n",
              "      <th>index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a child in a pink dress is climbing u...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a girl going into a wooden building ....</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a little girl climbing into a wooden ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a little girl climbing the stairs to ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "      <td>startseq a little girl in a pink dress going i...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c088c444-9dff-4751-a2ab-229210f384db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c088c444-9dff-4751-a2ab-229210f384db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c088c444-9dff-4751-a2ab-229210f384db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from copy import copy\n",
        "def add_start_end_seq_token(captions):\n",
        "    caps = []\n",
        "    for txt in captions:\n",
        "        txt = 'startseq ' + txt + ' endseq'\n",
        "        caps.append(txt)\n",
        "    return(caps)\n",
        "\n",
        "df_txt[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
        "df_txt.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhfZe4SJDGlo"
      },
      "source": [
        "# split the dataset int train and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MxqCDgSDDFKw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split your df_txt into train and test sets\n",
        "train_df, test_df = train_test_split(df_txt, test_size=0.2, shuffle=False)\n",
        "# dropping the imagefile that with shared captions in the two splits\n",
        "# this would also avoid data leakage\n",
        "test_df = test_df.iloc[1:]\n",
        "train_df =  train_df[:-4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3L06Z6A8aSl"
      },
      "source": [
        "# Image prepration\n",
        "## create features for image using InceptionV3 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GgffeHrI8d49",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from keras.models import Model\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "image_model = Model(inputs = base_model.input, outputs=base_model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O2ZHDosMOXWn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "import glob\n",
        "import pickle\n",
        "\n",
        "train_path = dir_Flickr_jpg\n",
        "path_all_images = glob.glob(train_path + '/*jpg')\n",
        "\n",
        "train_img = []  # list of all images in training set\n",
        "test_img = []\n",
        "for im in path_all_images:\n",
        "    file_name = os.path.basename(os.path.normpath(im))\n",
        "    # include images that only exist in the target directory\n",
        "    # can split the dataset this way\n",
        "    if(file_name in train_df.filename.to_list()):\n",
        "        train_img.append(im)\n",
        "    elif (file_name in test_df.filename.to_list()):\n",
        "        test_img.append(im)\n",
        "\n",
        "def preprocess(image_path):\n",
        "    # inception v3 excepts img in 299 * 299 * 3\n",
        "    image = load_img(image_path, target_size=(299, 299))\n",
        "    # convert the image pixels to a numpy array\n",
        "    x = img_to_array(image)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis = 0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "def encode(image_path):\n",
        "    image = preprocess(image_path)\n",
        "    vec = image_model.predict(image, verbose=0)\n",
        "    vec_flattened = vec.flatten()\n",
        "    return vec_flattened\n",
        "\n",
        "\n",
        "train_img_feats = {}\n",
        "test_img_feats = {}\n",
        "\n",
        "if not (os.path.exists('train_encoder.pkl') and os.path.exists('test_encoder.pkl')):\n",
        "    for image in train_img:\n",
        "        file_name = os.path.basename(os.path.normpath(image))\n",
        "        train_img_feats[file_name] = encode(image)\n",
        "    for image in test_img:\n",
        "        file_name = os.path.basename(os.path.normpath(image))\n",
        "        test_img_feats[file_name] = encode(image)\n",
        "    # Save the image features\n",
        "    with open('train_encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(train_img_feats, f)\n",
        "    with open('test_encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(test_img_feats, f)\n",
        "else:\n",
        "    # Load previously encoded image data\n",
        "    with open('train_encoder.pkl', 'rb') as f:\n",
        "        train_img_feats = pickle.load(f)\n",
        "    with open('test_encoder.pkl', 'rb') as f:\n",
        "        test_img_feats = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAoKpyScvQXD"
      },
      "source": [
        "# Tokenize the captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "tags": [],
        "id": "1YJRJTOOneuv"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "vocab_size = 8000\n",
        "# Tokenizer does not limit the number of words\n",
        "# it still finds all the words in the word_index\n",
        "# But it will only use the num_words given to encode the text in texts_to_sequences or sequences_to_texts methods\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df.caption.to_list())\n",
        "\n",
        "# get the word index\n",
        "train_seqs = tokenizer.texts_to_sequences(train_df.caption.to_list())\n",
        "test_seqs = tokenizer.texts_to_sequences(test_df.caption.to_list())\n",
        "\n",
        "# calculate the maximum caption length\n",
        "max_length = max(len(seq) for seq in train_seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ic_Utl6I4ev2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def data_generator(df, tokenizer, max_length, image_data, batch_size, generator_type):\n",
        "    num_batches = len(df) // batch_size\n",
        "    while True:\n",
        "        df = df.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
        "        for i in range(num_batches):\n",
        "            batch_df = df.iloc[i*batch_size:(i+1)*batch_size]\n",
        "            X1, X2, y = [], [], []\n",
        "            for index, row in batch_df.iterrows():\n",
        "                try:\n",
        "                    # try to get the image features from the image_data dictionary\n",
        "                    pic = image_data[row['filename']]\n",
        "                except KeyError:\n",
        "                    # if the file name is not found, print a warning message and skip this row\n",
        "                    print(f\"\"\"Warning ({generator_type} generator):\n",
        "                          file name {row['filename']} not found in image_data dictionary. Skipping this row.\"\"\")\n",
        "                    continue\n",
        "                seq = tokenizer.texts_to_sequences([row['caption']])[0]\n",
        "                for j in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:j], seq[j]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # out_seq = to_categorical([out_seq], num_classes=len(tokenizer.word_index)+1)[0]\n",
        "                    out_seq = [out_seq]\n",
        "                    X1.append(pic)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            yield [np.array(X1), np.array(X2)], np.array(y)\n",
        "\n",
        "batch_size = 64\n",
        "# create data generator for the train set\n",
        "train_generator = data_generator(train_df, tokenizer, max_length, train_img_feats, batch_size, generator_type=\"train\")\n",
        "\n",
        "# create data generator for the test set\n",
        "test_generator = data_generator(test_df, tokenizer, max_length, test_img_feats, batch_size, generator_type=\"val\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_LjvGtNAUUw"
      },
      "source": [
        "# Downloading GloVe to using its vector embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fddgWxT36kCN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Set the URL for the GloVe embeddings\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "\n",
        "# Set the path where the embeddings will be stored\n",
        "embeddings_dir = 'embeddings/glove'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(embeddings_dir):\n",
        "    os.makedirs(embeddings_dir)\n",
        "\n",
        "# Set the file name for the embeddings archive\n",
        "embeddings_zip = os.path.join(embeddings_dir, 'glove.6B.zip')\n",
        "\n",
        "# Download the embeddings archive if it doesn't exist\n",
        "if not os.path.exists(embeddings_zip):\n",
        "    print(f'Downloading GloVe embeddings from {url}...')\n",
        "    urllib.request.urlretrieve(url, embeddings_zip)\n",
        "    print('Done!')\n",
        "\n",
        "# Extract the embeddings if they haven't been extracted yet\n",
        "if not os.path.exists(os.path.join(embeddings_dir, 'glove.6B.100d.txt')):\n",
        "    print('Extracting GloVe embeddings...')\n",
        "    with zipfile.ZipFile(embeddings_zip, 'r') as zip_ref:\n",
        "        zip_ref.extractall(embeddings_dir)\n",
        "    print('Done!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rGfT12woASi-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# load GloVe embeddings\n",
        "embeddings_index = {}\n",
        "glove_path = \"./embeddings/glove/glove.6B.200d.txt\"\n",
        "\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# filter out the words that are not in the top num_words-1 most frequent words\n",
        "filtered_word_index = {word: i for word, i in tokenizer.word_index.items() if i < vocab_size}\n",
        "\n",
        "# prepare embedding matrix\n",
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in filtered_word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL5xf0ehChWB"
      },
      "source": [
        "# Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "V_3NT4N7CjxV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# define the model\n",
        "ip1 = layers.Input(shape = (2048, ))\n",
        "fe1 = layers.Dropout(0.2)(ip1)\n",
        "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
        "ip2 = layers.Input(shape = (max_length, ))\n",
        "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
        "se2 = layers.Dropout(0.2)(se1)\n",
        "se3 = layers.LSTM(256)(se2)\n",
        "decoder1 = layers.add([fe2, se3])\n",
        "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
        "outputs = layers.Dense(vocab_size)(decoder2)\n",
        "model = Model(inputs = [ip1, ip2], outputs = outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPygHAUZDIyn"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNtuCFkNneu0",
        "outputId": "f81e9357-bd39-486f-e725-e884338e45fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "505/505 [==============================] - 102s 183ms/step - loss: 3.8765 - sparse_categorical_accuracy: 0.3090 - val_loss: 3.5089 - val_sparse_categorical_accuracy: 0.3460\n",
            "Epoch 2/5\n",
            "505/505 [==============================] - 87s 172ms/step - loss: 3.1862 - sparse_categorical_accuracy: 0.3675 - val_loss: 3.2745 - val_sparse_categorical_accuracy: 0.3733\n",
            "Epoch 3/5\n",
            "505/505 [==============================] - 86s 170ms/step - loss: 2.8742 - sparse_categorical_accuracy: 0.3936 - val_loss: 3.2136 - val_sparse_categorical_accuracy: 0.3848\n",
            "Epoch 4/5\n",
            "505/505 [==============================] - 87s 172ms/step - loss: 2.6741 - sparse_categorical_accuracy: 0.4102 - val_loss: 3.2006 - val_sparse_categorical_accuracy: 0.3901\n",
            "Epoch 5/5\n",
            "505/505 [==============================] - 85s 168ms/step - loss: 2.5178 - sparse_categorical_accuracy: 0.4233 - val_loss: 3.2296 - val_sparse_categorical_accuracy: 0.3932\n"
          ]
        }
      ],
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False\n",
        "\n",
        "# define loss function\n",
        "def sparse_loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = tf.keras.losses.sparse_categorical_crossentropy(real, pred, from_logits=True) # add from_logits argument\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "model.compile(loss = sparse_loss_function, optimizer = 'adam',\n",
        "               metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "num_batches = len(train_df) // batch_size\n",
        "num_val_batches = len(test_df) // batch_size\n",
        "\n",
        "hist = model.fit(train_generator,\n",
        "          validation_data=test_generator,\n",
        "          validation_steps=num_val_batches,\n",
        "          epochs = 5,\n",
        "          steps_per_epoch=num_batches,\n",
        "          verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inSz8Ua2TM6P",
        "tags": []
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_YvQNHyTDhY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define some custom metadata for the model\n",
        "metadata = {\n",
        "  'name': 'image_caption_generator',\n",
        "  'description': 'A model that generates captions for images using InceptionV3 and have all the stop words, and with no lemmatization',\n",
        "  'parameters': {\n",
        "    'vocab_size': 8000,\n",
        "    'embedding_dim': 200,\n",
        "    'lstm_units': 256,\n",
        "    'beam_size': 5\n",
        "  },\n",
        "  'performance': {\n",
        "    'loss': 1.6700,\n",
        "    'accuracy': 'Nan',\n",
        "    'bleu_score': 'NaN'\n",
        "  }\n",
        "}\n",
        "\n",
        "# save the model with the metadata\n",
        "model.save('capGen_model_v2.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D02QybmdJ3F2"
      },
      "outputs": [],
      "source": [
        "model.save('img_cap_v1.5.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU Score"
      ],
      "metadata": {
        "id": "bQnHtzf5tnjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caption_eval(image_vec):\n",
        "    image_vec = image_vec.reshape(1, -1)\n",
        "    # initialize the caption with the start token\n",
        "    caption = [tokenizer.word_index['startseq']]\n",
        "    # loop until the end token or the maximum length is reached\n",
        "\n",
        "    for i in range(max_length):\n",
        "        padded_caption = pad_sequences([caption], maxlen=max_length, padding='post')\n",
        "        # predict the next word using the model\n",
        "        prediction = model.predict([image_vec, padded_caption], verbose=0)\n",
        "        # get the word with the highest probability\n",
        "        word_index = np.argmax(prediction)\n",
        "        # append the word to the caption\n",
        "        caption.append(word_index)\n",
        "        # break if the end token is reached\n",
        "        if word_index == tokenizer.word_index['endseq']:\n",
        "            break\n",
        "    # convert the caption indices to words\n",
        "    caption_words = [tokenizer.index_word[i] for i in caption]\n",
        "    # join the words to form a sentence\n",
        "    caption_sentence = ' '.join(caption_words[1:-1])\n",
        "    return caption_sentence"
      ],
      "metadata": {
        "id": "j7MOHYaIRmLK"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_word = tokenizer.index_word\n",
        "\n",
        "nkeep = 5\n",
        "pred_good, pred_bad, bleus = [], [], [] \n",
        "count = 0 \n",
        "# fname: filenames from the dataframe\n",
        "# dt_test: tokenized text from tokenizer.texts_to_sequences()\n",
        "# di_test: image feature arrays\n",
        "test_img_feats_array = np.array(list(test_img_feats.values()))\n",
        "for jpgfnm, image_feature, tokenized_text in zip(test_df.filename.to_list(), test_img_feats_array, test_seqs):\n",
        "    count += 1\n",
        "    if count % 200 == 0:\n",
        "        print(\"  {:4.2f}% is done..\".format(100*count/float(len(test_df))))\n",
        "\n",
        "    caption_true = [ index_word[i] for i in tokenized_text ]     \n",
        "    caption_true = caption_true[1:-1] ## remove startseq, and endseq\n",
        "    ## captions\n",
        "    caption = predict_caption_eval(image_feature)\n",
        "    caption = caption.split()\n",
        "\n",
        "    bleu = sentence_bleu([caption_true],caption)\n",
        "    bleus.append(bleu)\n",
        "    if bleu > 0.7 and len(pred_good) < nkeep:\n",
        "        pred_good.append((bleu,jpgfnm,caption_true,caption))\n",
        "    elif bleu < 0.3 and len(pred_bad) < nkeep:\n",
        "        pred_bad.append((bleu,jpgfnm,caption_true,caption))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51l6uG3gQFwm",
        "outputId": "2085e581-6641-4aa6-fab2-d888d53f8453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.9/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  2.47% is done..\n",
            "  4.94% is done..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caption(image_path):\n",
        "  # encode the image\n",
        "  image_vec = encode(image_path)\n",
        "  image_vec = image_vec.reshape(1, -1)\n",
        "  # initialize the caption with the start token\n",
        "  caption = [tokenizer.word_index['startseq']]\n",
        "  # loop until the end token or the maximum length is reached\n",
        "\n",
        "  for i in range(max_length):\n",
        "    padded_caption = pad_sequences([caption], maxlen=max_length, padding='post')\n",
        "    # predict the next word using the model\n",
        "    if i == 2:\n",
        "        print(f\"image vec: {image_vec}\\n image shape: {image_vec.shape}\")\n",
        "    prediction = model.predict([image_vec, padded_caption], verbose=0)\n",
        "    # get the word with the highest probability\n",
        "    word_index = np.argmax(prediction)\n",
        "    # append the word to the caption\n",
        "    caption.append(word_index)\n",
        "    # break if the end token is reached\n",
        "    if word_index == tokenizer.word_index['endseq']:\n",
        "      break\n",
        "  # convert the caption indices to words\n",
        "  caption_words = [tokenizer.index_word[i] for i in caption]\n",
        "  # join the words to form a sentence\n",
        "  caption_sentence = ' '.join(caption_words[1:-1])\n",
        "  return caption_sentence\n",
        "\n",
        "generated_caption = predict_caption(\"/content/test_image.jpg\")\n",
        "print(generated_caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YvZZl5vMqaW",
        "outputId": "9d46a540-7150-44e2-8537-fe3c73868c21"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image vec: [[0.3857699  0.8185491  0.7321446  ... 0.40684837 0.6541158  0.64061016]]\n",
            " image shape: (1, 2048)\n",
            "a dog is jumping over a red and white toy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting a caption for new image\n",
        "## greedy search and beam serach"
      ],
      "metadata": {
        "id": "O92EHaaMt5nL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-r8I_m6Aneu4"
      },
      "outputs": [],
      "source": [
        "def beam_search_caption(image_path, beam_width):\n",
        "    # encode the image\n",
        "    image_vec = encode(image_path)\n",
        "    image_vec = image_vec.reshape(1, -1)\n",
        "    # initialize the caption with the start token\n",
        "    caption = [vectorizer([\"startseq\"]).numpy()[0][0]]\n",
        "    # initialize beam search\n",
        "    beam = [(caption, 0)]\n",
        "    \n",
        "    # loop until the end token or the maximum length is reached\n",
        "    for i in range(max_length):\n",
        "        # generate new candidates\n",
        "        candidates = []\n",
        "        for j in range(len(beam)):\n",
        "            seq, score = beam[j]\n",
        "            # check if the sequence ends with endseq\n",
        "            if seq[-1] == vectorizer_word_index[\"endseq\"]:\n",
        "                candidates.append((seq, score))\n",
        "                continue\n",
        "            # predict the next word using the model\n",
        "            padded_caption = pad_sequences([seq], maxlen=max_length, padding='post')\n",
        "            prediction = model.predict([image_vec, padded_caption], verbose=0)[0]\n",
        "            # get the top k words with the highest probability\n",
        "            top_k = prediction.argsort()[-beam_width:][::-1]\n",
        "            # add new candidates to the list\n",
        "            for w in top_k:\n",
        "                new_seq = seq + [w]\n",
        "                new_score = score + np.log(prediction[w])\n",
        "                candidates.append((new_seq, new_score))\n",
        "        # select top k candidates\n",
        "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "        \n",
        "    # select the best candidate\n",
        "    seq, score = beam[0]\n",
        "    # convert the caption indices to words\n",
        "    caption_words = [vectorizer_index_word[i] for i in seq]\n",
        "    # join the words to form a sentence\n",
        "    caption_sentence = ' '.join(caption_words[1:-1])\n",
        "    return caption_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d0r5UjhV8Q-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "generated_caption = beam_search_caption(\"test_img.jpg\", 10)\n",
        "print(generated_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cffdeMRmHEb"
      },
      "source": [
        "# Model 2, more complicated\n",
        "* You can use a bidirectional LSTM instead of a single LSTM for the caption encoder. This way, you can capture the context from both directions of the caption sequence, and generate more coherent captions.\n",
        "\n",
        "* You can use an attention mechanism to allow the decoder to focus on different parts of the image and the caption encoder outputs at each time step. This way, you can generate more relevant and informative captions that align with the image content.\n",
        "\n",
        "* You can use a scheduled sampling technique to train the decoder with a mix of ground truth and predicted words. This way, you can reduce the exposure bias and improve the generalization ability of the decoder.\n",
        "\n",
        "* You can use a beam search instead of a greedy search for generating captions. This way, you can explore more possible captions and choose the one with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93vaiXH4mJx5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Attention\n",
        "\n",
        "# define the model\n",
        "ip1 = layers.Input(shape = (2048, ))\n",
        "fe1 = layers.Dropout(0.2)(ip1)\n",
        "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
        "ip2 = layers.Input(shape = (max_length, ))\n",
        "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
        "se2 = layers.Dropout(0.2)(se1)\n",
        "se3 = Bidirectional(layers.LSTM(256, return_sequences=True))(se2) # use bidirectional LSTM\n",
        "decoder1 = layers.add([fe2, se3[:, -1]]) # use last hidden state of bidirectional LSTM\n",
        "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
        "attn_layer = Attention() # use attention layer\n",
        "context_vector, attention_weights = attn_layer([decoder2, se3]) # get context vector and attention weights\n",
        "decoder3 = layers.Dense(256, activation='relu')(context_vector) # use context vector for final dense layer\n",
        "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder3)\n",
        "model_v2 = Model(inputs = [ip1, ip2], outputs = outputs)\n",
        "\n",
        "model_v2.layers[3].set_weights([embedding_matrix])\n",
        "model_v2.layers[3].trainable = False\n",
        "model_v2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "num_batches = len(df_txt0) // batch_size\n",
        "model_v2.fit(train_generator, epochs = 50, steps_per_epoch=num_batches, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UXfPrCcmxsq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ6uH3DimysF"
      },
      "source": [
        "## Beam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgdmGhyLmy8M"
      },
      "outputs": [],
      "source": [
        "def beam_search(image_path, beam_size):\n",
        "  # encode the image\n",
        "  image_vec = encode(image_path)\n",
        "  # add another dimension to match the model input\n",
        "  image_vec = np.expand_dims(image_vec, axis=0)\n",
        "  # initialize the candidates with the start token\n",
        "  candidates = [[tokenizer.word_index['<start>']]]\n",
        "  # initialize the probabilities with 1\n",
        "  probabilities = [1]\n",
        "  # loop until the maximum length is reached\n",
        "  for i in range(max_length):\n",
        "    # initialize a list to store the next candidates\n",
        "    next_candidates = []\n",
        "    # initialize a list to store the next probabilities\n",
        "    next_probabilities = []\n",
        "    # loop over the current candidates\n",
        "    for j in range(len(candidates)):\n",
        "      # get the current candidate\n",
        "      candidate = candidates[j]\n",
        "      # pad the candidate sequence\n",
        "      padded_candidate = pad_sequences([candidate], maxlen=max_length, padding='post')\n",
        "      # predict the next word using the model\n",
        "      prediction = model.predict([image_vec, padded_candidate], verbose=0)\n",
        "      # get the top beam_size words and their probabilities\n",
        "      top_words = np.argsort(prediction[0])[-beam_size:]\n",
        "      top_probs = prediction[0][top_words]\n",
        "      # loop over the top words and their probabilities\n",
        "      for k in range(beam_size):\n",
        "        # get the word and its probability\n",
        "        word = top_words[k]\n",
        "        prob = top_probs[k]\n",
        "        # append the word to the candidate and multiply the probability\n",
        "        next_candidate = candidate + [word]\n",
        "        next_prob = probabilities[j] * prob\n",
        "        # append the next candidate and probability to the lists\n",
        "        next_candidates.append(next_candidate)\n",
        "        next_probabilities.append(next_prob)\n",
        "    # sort the next candidates and probabilities by descending order of probability\n",
        "    sorted_indices = np.argsort(next_probabilities)[::-1]\n",
        "    sorted_candidates = [next_candidates[i] for i in sorted_indices]\n",
        "    sorted_probabilities = [next_probabilities[i] for i in sorted_indices]\n",
        "    # select the top beam_size candidates and probabilities for the next iteration\n",
        "    candidates = sorted_candidates[:beam_size]\n",
        "    probabilities = sorted_probabilities[:beam_size]\n",
        "    # check if any candidate has reached the end token\n",
        "    end_index = tokenizer.word_index['<end>']\n",
        "    if any(candidate[-1] == end_index for candidate in candidates):\n",
        "      break\n",
        "  # return the candidate with the highest probability\n",
        "  best_candidate = candidates[0]\n",
        "  # convert the candidate indices to words\n",
        "  caption_words = [tokenizer.index_word[i] for i in best_candidate]\n",
        "  # join the words to form a sentence\n",
        "  caption_sentence = ' '.join(caption_words[1:-1])\n",
        "  return caption_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSA86kO8z39g",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "def data_generator(df, image_data, batch_size):\n",
        "    # create a dataset from the data frame\n",
        "    ds = tf.data.Dataset.from_tensor_slices((df['filename'], df['caption']))\n",
        "    # map the filename to the image data\n",
        "    ds = ds.map(lambda x, y: (image_data[x], y))\n",
        "    # apply the TextVectorization layer as a transformation\n",
        "    ds = ds.map(lambda x, y: (x, vectorizer([y])))\n",
        "    # unbatch the dataset to get individual elements\n",
        "    ds = ds.unbatch()\n",
        "    # create input and output sequences\n",
        "    ds = ds.map(lambda x, y: (x, y[:-1], y[1:]))\n",
        "    # pad and one-hot encode the sequences if needed\n",
        "    # ds = ds.map(lambda x, y, z: (x, pad_sequences([y], maxlen=max_length)[0], to_categorical([z], num_classes=vocab_size)[0]))\n",
        "    # batch the dataset\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds\n",
        "\n",
        "batch_size = 64\n",
        "train_generator = data_generator(df_txt, image_data, batch_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "tf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}