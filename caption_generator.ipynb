{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6Xkro5uPCo_",
    "outputId": "f483fc8c-8847-4e5a-ef07-ecf5b3fe0e52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.9.16 (main, Mar  8 2023, 14:00:05) \n",
      "[GCC 11.2.0]\n",
      "keras version 2.12.0\n",
      "tensorflow version 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys, time, os, warnings \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from collections import Counter \n",
    "\n",
    "print(\"python {}\".format(sys.version))\n",
    "print(\"keras version {}\".format(keras.__version__)); del keras\n",
    "print(\"tensorflow version {}\".format(tf.__version__))\n",
    "\n",
    "def set_seed(sd=123):\n",
    "    from numpy.random import seed\n",
    "    from tensorflow import set_random_seed\n",
    "    import random as rn\n",
    "    ## numpy random seed\n",
    "    seed(sd)\n",
    "    ## core python's random number \n",
    "    rn.seed(sd)\n",
    "    ## tensor flow's random number\n",
    "    set_random_seed(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mu7zkvTxPT-9",
    "outputId": "0ff63aed-e7da-4f3d-a16d-e1f249a796d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/mirshahi/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Requirement already satisfied: opendatasets in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (0.1.22)\n",
      "Requirement already satisfied: kaggle in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from opendatasets) (1.5.13)\n",
      "Requirement already satisfied: tqdm in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from opendatasets) (4.65.0)\n",
      "Requirement already satisfied: click in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from opendatasets) (8.1.3)\n",
      "Requirement already satisfied: requests in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle->opendatasets) (2.28.2)\n",
      "Requirement already satisfied: python-dateutil in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: certifi in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle->opendatasets) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.10 in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: python-slugify in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle->opendatasets) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from kaggle->opendatasets) (1.26.15)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->kaggle->opendatasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mirshahi/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->kaggle->opendatasets) (3.4)\n",
      "Skipping, found downloaded files in \"./flickr8k\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets\n",
    "import opendatasets as od\n",
    "import pandas\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/adityajn105/flickr8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNw0Wid3Vlpu",
    "outputId": "be4810b0-86f2-4fce-fce2-96a0c0926195",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of jpg flies in Flicker8k: 8091\n"
     ]
    }
   ],
   "source": [
    "## The location of the Flickr8K_ photos\n",
    "dir_Flickr_jpg = \"./flickr8k/Images\"\n",
    "## The location of the caption file\n",
    "dir_Flickr_text = \"./flickr8k/captions.txt\"\n",
    "\n",
    "jpgs = os.listdir(dir_Flickr_jpg)\n",
    "print(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45SpOt_GXL-u"
   },
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YVHrZfSYxmO",
    "outputId": "5cb5f94a-4a47-4d00-e468-0b7f4271f559",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique file names : 8091\n",
      "The distribution of the number of captions for each image:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({5: 8091})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txt = pd.read_csv(dir_Flickr_text, skiprows=1, names=[\"filename\", \"caption\"])\n",
    "df_txt['caption'] = df_txt['caption'].str.lower()\n",
    "\n",
    "df_txt['index'] = df_txt.groupby(\"filename\").cumcount()\n",
    "\n",
    "uni_filenames = np.unique(df_txt.filename.values)\n",
    "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
    "print(\"The distribution of the number of captions for each image:\")\n",
    "Counter(Counter(df_txt.filename.values).values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OEq8DiKoR_Y"
   },
   "source": [
    "# Data prepration\n",
    "prepare text and image separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HMihehhY6Ps7",
    "outputId": "206f38ca-5c32-4af2-df9f-a989007cb741",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a child in a pink dress is climbing u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a girl going into a wooden building ....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a little girl climbing into a wooden ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a little girl climbing the stairs to ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>startseq a little girl in a pink dress going i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    filename   \n",
       "0  1000268201_693b08cb0e.jpg  \\\n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  index  \n",
       "0  startseq a child in a pink dress is climbing u...      0  \n",
       "1  startseq a girl going into a wooden building ....      1  \n",
       "2  startseq a little girl climbing into a wooden ...      2  \n",
       "3  startseq a little girl climbing the stairs to ...      3  \n",
       "4  startseq a little girl in a pink dress going i...      4  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import copy\n",
    "def add_start_end_seq_token(captions):\n",
    "    caps = []\n",
    "    for txt in captions:\n",
    "        txt = 'startseq ' + txt + ' endseq'\n",
    "        caps.append(txt)\n",
    "    return(caps)\n",
    "\n",
    "df_txt[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
    "df_txt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfZe4SJDGlo"
   },
   "source": [
    "# split the dataset int train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "MxqCDgSDDFKw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split your df_txt into train and test sets\n",
    "train_df, test_df = train_test_split(df_txt, test_size=0.2, shuffle=False)\n",
    "# dropping the imagefile that with shared captions in the two splits\n",
    "# this would also avoid data leakage\n",
    "test_df = test_df.iloc[1:]\n",
    "train_df =  train_df[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3L06Z6A8aSl"
   },
   "source": [
    "# Image prepration\n",
    "## create features for image using InceptionV3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "GgffeHrI8d49",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "image_model = Model(inputs = base_model.input, outputs=base_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "O2ZHDosMOXWn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "train_path = dir_Flickr_jpg\n",
    "path_all_images = glob.glob(train_path + '/*jpg')\n",
    "\n",
    "train_img = []  # list of all images in training set\n",
    "test_img = []\n",
    "for im in path_all_images:\n",
    "    file_name = os.path.basename(os.path.normpath(im))\n",
    "    # include images that only exist in the target directory\n",
    "    # can split the dataset this way\n",
    "    if(file_name in df_train.filename.to_list()):\n",
    "        train_img.append(im)\n",
    "    elif (file_name in df_test.filename.to_list()):\n",
    "        test_img.append(im)\n",
    "\n",
    "def preprocess(image_path):\n",
    "    # inception v3 excepts img in 299 * 299 * 3\n",
    "    image = load_img(image_path, target_size=(299, 299))\n",
    "    # convert the image pixels to a numpy array\n",
    "    x = img_to_array(image)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "def encode(image_path):\n",
    "    image = preprocess(image_path)\n",
    "    vec = image_model.predict(image, verbose=0)\n",
    "    vec_flattened = vec.flatten()\n",
    "    return vec_flattened\n",
    "\n",
    "\n",
    "train_img_feats = {}\n",
    "test_img_feats = {}\n",
    "\n",
    "if not (os.path.exists('train_encoder.pkl') and os.path.exists('test_encoder.pkl')):\n",
    "    for image in train_img:\n",
    "        file_name = os.path.basename(os.path.normpath(image))\n",
    "        train_img_feats[file_name] = encode(image)\n",
    "    for image in test_img:\n",
    "        file_name = os.path.basename(os.path.normpath(image))\n",
    "        test_img_feats[file_name] = encode(image)\n",
    "    # Save the image features\n",
    "    with open('train_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(train_img_feats, f)\n",
    "    with open('test_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(test_img_feats, f)\n",
    "else:\n",
    "    # Load previously encoded image data\n",
    "    with open('train_encoder.pkl', 'rb') as f:\n",
    "        train_img_feats = pickle.load(f)\n",
    "    with open('test_encoder.pkl', 'rb') as f:\n",
    "        test_img_feats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAoKpyScvQXD"
   },
   "source": [
    "# Tokenize the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O06giiUErwmI",
    "outputId": "feb9453f-1942-4503-d3c3-bc053c047b89",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vocab_size = 8000\n",
    "# output_sequence_length = 20 # adjust this according to your max_length\n",
    "\n",
    "# create the TextVectorization layer\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode=\"int\")\n",
    "\n",
    "# adapt the layer to the captions_list\n",
    "vectorizer.adapt(df_train.caption.to_list())\n",
    "\n",
    "train_seqs = vectorizer(df_train.caption.to_list())\n",
    "test_seqs = vectorizer(df_test.caption.to_list())\n",
    "\n",
    "vectorizer_word_index = {}\n",
    "vectorizer_index_word = {}\n",
    "for index, word in enumerate(vectorizer.get_vocabulary()):\n",
    "    if index == 0:\n",
    "        continue\n",
    "    vectorizer_word_index[word] = index\n",
    "    vectorizer_index_word[index] = word\n",
    "\n",
    "# get the max length of sequences from the vectorizer\n",
    "max_length = train_seqs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ic_Utl6I4ev2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def data_generator(df, vectorizer, max_length, vocab_size, image_data, batch_size):\n",
    "    num_batches = len(df) // batch_size\n",
    "    while True:\n",
    "        for i in range(num_batches):\n",
    "            batch_df = df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            X1, X2, y = [], [], []\n",
    "            for index, row in batch_df.iterrows():\n",
    "                try:\n",
    "                    # try to get the image features from the image_data dictionary\n",
    "                    pic = image_data[row['filename']]\n",
    "                except KeyError:\n",
    "                    # if the file name is not found, print a warning message and skip this row\n",
    "                    print(f\"Warning: file name {row['filename']} not found in image_data dictionary. Skipping this row.\")\n",
    "                    continue\n",
    "                seq = vectorizer([row['caption']]).numpy()[0] # convert caption to vectorized tensor and then to numpy array\n",
    "                # seq = np.argmax(seq) # optional: get index of max value in vector\n",
    "                for j in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:j], seq[j]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    X1.append(pic)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "\n",
    "batch_size = 64\n",
    "# create data generator for the train set\n",
    "train_generator = data_generator(df_train, vectorizer, max_length, vocab_size, train_img_feats, batch_size)\n",
    "\n",
    "# create data generator for the test set\n",
    "test_generator = data_generator(df_test, vectorizer, max_length, vocab_size, test_img_feats, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic: [0.12277637 0.33293095 0.7527249  ... 0.21941511 0.3020853  0.40279624], filename: 1000268201_693b08cb0e.jpg\n",
      "seq: [   3    2   45    5    2   96  177    8  124   52    2  417   13  383\n",
      "    5   30 7042  637    4], seq.shape: (19,)\n",
      "pic: [0.12277637 0.33293095 0.7527249  ... 0.21941511 0.3020853  0.40279624], filename: 1000268201_693b08cb0e.jpg\n",
      "seq: [  3   2  21 312  67   2 195 120   4], seq.shape: (9,)\n",
      "pic: [0.12277637 0.33293095 0.7527249  ... 0.21941511 0.3020853  0.40279624], filename: 1000268201_693b08cb0e.jpg\n",
      "seq: [   3    2   41   21  124   67    2  195 2249    4], seq.shape: (10,)\n",
      "pic: [0.12277637 0.33293095 0.7527249  ... 0.21941511 0.3020853  0.40279624], filename: 1000268201_693b08cb0e.jpg\n",
      "seq: [   3    2   41   21  124    6  383   20   63 2249    4], seq.shape: (11,)\n",
      "pic: [0.12277637 0.33293095 0.7527249  ... 0.21941511 0.3020853  0.40279624], filename: 1000268201_693b08cb0e.jpg\n",
      "seq: [   3    2   41   21    5    2   96  177  312   67    2  195 3010    4], seq.shape: (14,)\n",
      "pic: [0.7366835  0.5911317  0.18194187 ... 0.16030255 0.34144703 0.9083163 ], filename: 1001773457_577c3a7d70.jpg\n",
      "seq: [  3   2  16  10   9   2 848  10  18 374   4], seq.shape: (11,)\n"
     ]
    }
   ],
   "source": [
    "test_df = df_txt[:6]\n",
    "for index, row in test_df.iterrows():\n",
    "    pic = image_data[row['filename']]\n",
    "    print(f\"pic: {pic}, filename: {row['filename']}\")\n",
    "    seq = vectorizer([row['caption']]).numpy()[0]\n",
    "    print(f\"seq: {seq}, seq.shape: {seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_LjvGtNAUUw"
   },
   "source": [
    "# Downloading GloVe to using its vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fddgWxT36kCN",
    "outputId": "445aa7e5-efe3-4493-cc17-eb43ecb71942",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe embeddings from http://nlp.stanford.edu/data/glove.6B.zip...\n",
      "Done!\n",
      "Extracting GloVe embeddings...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Set the URL for the GloVe embeddings\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "\n",
    "# Set the path where the embeddings will be stored\n",
    "embeddings_dir = 'embeddings/glove'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(embeddings_dir):\n",
    "    os.makedirs(embeddings_dir)\n",
    "\n",
    "# Set the file name for the embeddings archive\n",
    "embeddings_zip = os.path.join(embeddings_dir, 'glove.6B.zip')\n",
    "\n",
    "# Download the embeddings archive if it doesn't exist\n",
    "if not os.path.exists(embeddings_zip):\n",
    "    print(f'Downloading GloVe embeddings from {url}...')\n",
    "    urllib.request.urlretrieve(url, embeddings_zip)\n",
    "    print('Done!')\n",
    "\n",
    "# Extract the embeddings if they haven't been extracted yet\n",
    "if not os.path.exists(os.path.join(embeddings_dir, 'glove.6B.100d.txt')):\n",
    "    print('Extracting GloVe embeddings...')\n",
    "    with zipfile.ZipFile(embeddings_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(embeddings_dir)\n",
    "    print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rGfT12woASi-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load GloVe embeddings\n",
    "embeddings_index = {}\n",
    "glove_path = \"./embeddings/glove/glove.6B.200d.txt\"\n",
    "\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# prepare embedding matrix\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for i, word in enumerate(word_list):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL5xf0ehChWB"
   },
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "V_3NT4N7CjxV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# define the model\n",
    "ip1 = layers.Input(shape = (2048, ))\n",
    "fe1 = layers.Dropout(0.2)(ip1)\n",
    "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
    "ip2 = layers.Input(shape = (max_length, ))\n",
    "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
    "se2 = layers.Dropout(0.2)(se1)\n",
    "se3 = layers.LSTM(256)(se2)\n",
    "decoder1 = layers.add([fe2, se3])\n",
    "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
    "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder2)\n",
    "model2 = Model(inputs = [ip1, ip2], outputs = outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPygHAUZDIyn"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:32:01.691560: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - ETA: 0s - loss: 4.4238Warning: file name 1000268201_693b08cb0e.jpg not found in image_data dictionary. Skipping this row.\n",
      "Warning: file name 1000268201_693b08cb0e.jpg not found in image_data dictionary. Skipping this row.\n",
      "Warning: file name 1000268201_693b08cb0e.jpg not found in image_data dictionary. Skipping this row.\n",
      "Warning: file name 1000268201_693b08cb0e.jpg not found in image_data dictionary. Skipping this row.\n",
      "Warning: file name 1000268201_693b08cb0e.jpg not found in image_data dictionary. Skipping this row.\n",
      "Warning: file name 1001773457_577c3a7d70.jpg not found in image_data dictionary. Skipping this row.\n",
      "505/505 [==============================] - 241s 472ms/step - loss: 4.4238\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:36:02.385809: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 237s 468ms/step - loss: 3.5119\n",
      "Epoch 3/50\n",
      "505/505 [==============================] - 236s 468ms/step - loss: 3.1816\n",
      "Epoch 4/50\n",
      "505/505 [==============================] - 237s 469ms/step - loss: 2.9600\n",
      "Epoch 5/50\n",
      " 94/505 [====>.........................] - ETA: 3:12 - loss: 2.8720"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[216], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[1;32m      6\u001b[0m num_val_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_val_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.layers[2].set_weights([embedding_matrix])\n",
    "model2.layers[2].trainable = False\n",
    "model2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "num_batches = len(train_df) // batch_size\n",
    "num_val_batches = len(test_df) // batch_size\n",
    "\n",
    "model2.fit(train_generator,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=num_val_batches,\n",
    "          epochs = 50,\n",
    "          steps_per_epoch=num_batches,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "ESmmmVAjDLla",
    "outputId": "088b6c89-fe93-4230-c047-5e0b761007bc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:48:57.683887: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 242s 475ms/step - loss: 1.7030\n",
      "Epoch 2/50\n",
      "505/505 [==============================] - 238s 471ms/step - loss: 1.6615\n",
      "Epoch 3/50\n",
      "505/505 [==============================] - 238s 472ms/step - loss: 1.6531\n",
      "Epoch 4/50\n",
      "505/505 [==============================] - 233s 461ms/step - loss: 1.6478\n",
      "Epoch 5/50\n",
      "505/505 [==============================] - 233s 461ms/step - loss: 1.6430\n",
      "Epoch 6/50\n",
      "505/505 [==============================] - 233s 461ms/step - loss: 1.6356\n",
      "Epoch 7/50\n",
      "505/505 [==============================] - 233s 461ms/step - loss: 1.6326\n",
      "Epoch 8/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.6264\n",
      "Epoch 9/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.6224\n",
      "Epoch 10/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.6161\n",
      "Epoch 11/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.6132\n",
      "Epoch 12/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.6104\n",
      "Epoch 13/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.6059\n",
      "Epoch 14/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.6017\n",
      "Epoch 15/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5991\n",
      "Epoch 16/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5945\n",
      "Epoch 17/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5915\n",
      "Epoch 18/50\n",
      "505/505 [==============================] - 233s 461ms/step - loss: 1.5881\n",
      "Epoch 19/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5848\n",
      "Epoch 20/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5822\n",
      "Epoch 21/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5793\n",
      "Epoch 22/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5757\n",
      "Epoch 23/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5756\n",
      "Epoch 24/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5687\n",
      "Epoch 25/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5653\n",
      "Epoch 26/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5626\n",
      "Epoch 27/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5610\n",
      "Epoch 28/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5556\n",
      "Epoch 29/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5548\n",
      "Epoch 30/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5519\n",
      "Epoch 31/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5490\n",
      "Epoch 32/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5448\n",
      "Epoch 33/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5444\n",
      "Epoch 34/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5451\n",
      "Epoch 35/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5402\n",
      "Epoch 36/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5375\n",
      "Epoch 37/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5335\n",
      "Epoch 38/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5302\n",
      "Epoch 39/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5284\n",
      "Epoch 40/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5283\n",
      "Epoch 41/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5256\n",
      "Epoch 42/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5244\n",
      "Epoch 43/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5216\n",
      "Epoch 44/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5163\n",
      "Epoch 45/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5171\n",
      "Epoch 46/50\n",
      "505/505 [==============================] - 232s 459ms/step - loss: 1.5155\n",
      "Epoch 47/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5145\n",
      "Epoch 48/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5142\n",
      "Epoch 49/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5104\n",
      "Epoch 50/50\n",
      "505/505 [==============================] - 232s 460ms/step - loss: 1.5082\n"
     ]
    }
   ],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "num_batches = len(train_df) // batch_size\n",
    "num_val_batches = len(test_df) // batch_size\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "          validation_data=test_generator,\n",
    "          validation_steps=num_val_batches,\n",
    "          epochs = 50,\n",
    "          steps_per_epoch=num_batches,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.702988862991333,\n",
       "  1.661457896232605,\n",
       "  1.6530652046203613,\n",
       "  1.6478289365768433,\n",
       "  1.6429641246795654,\n",
       "  1.6355918645858765,\n",
       "  1.6325743198394775,\n",
       "  1.6264339685440063,\n",
       "  1.622445821762085,\n",
       "  1.616100549697876,\n",
       "  1.6131505966186523,\n",
       "  1.6103819608688354,\n",
       "  1.605942726135254,\n",
       "  1.6016764640808105,\n",
       "  1.5991183519363403,\n",
       "  1.5945446491241455,\n",
       "  1.5914604663848877,\n",
       "  1.5880521535873413,\n",
       "  1.5848066806793213,\n",
       "  1.5822334289550781,\n",
       "  1.5793291330337524,\n",
       "  1.5757285356521606,\n",
       "  1.575554609298706,\n",
       "  1.5686756372451782,\n",
       "  1.5653257369995117,\n",
       "  1.5625778436660767,\n",
       "  1.5610054731369019,\n",
       "  1.5556482076644897,\n",
       "  1.554849624633789,\n",
       "  1.551855206489563,\n",
       "  1.5489826202392578,\n",
       "  1.5448027849197388,\n",
       "  1.5443841218948364,\n",
       "  1.5451078414916992,\n",
       "  1.5401510000228882,\n",
       "  1.5375490188598633,\n",
       "  1.5335190296173096,\n",
       "  1.530239224433899,\n",
       "  1.5283831357955933,\n",
       "  1.5282753705978394,\n",
       "  1.5256048440933228,\n",
       "  1.5243650674819946,\n",
       "  1.5215511322021484,\n",
       "  1.5163370370864868,\n",
       "  1.5171172618865967,\n",
       "  1.5155068635940552,\n",
       "  1.5144888162612915,\n",
       "  1.5142102241516113,\n",
       "  1.5104057788848877,\n",
       "  1.5082296133041382]}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inSz8Ua2TM6P",
    "tags": []
   },
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "U_YvQNHyTDhY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some custom metadata for the model\n",
    "metadata = {\n",
    "  'name': 'image_caption_generator',\n",
    "  'description': 'A model that generates captions for images using InceptionV3 and have all the stop words, and with no lemmatization',\n",
    "  'parameters': {\n",
    "    'vocab_size': 8000,\n",
    "    'embedding_dim': 200,\n",
    "    'lstm_units': 256,\n",
    "    'beam_size': 5\n",
    "  },\n",
    "  'performance': {\n",
    "    'loss': 1.6700,\n",
    "    'accuracy': 'Nan',\n",
    "    'bleu_score': 'NaN'\n",
    "  }\n",
    "}\n",
    "\n",
    "# save the model with the metadata\n",
    "model.save('capGen_model_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D02QybmdJ3F2"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('img_cap_model_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_old = tf.keras.models.load_model(\"./caption_generator_inceptionV3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "w-BydfZoYSi7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a black and white dog is running though a black and white ball'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the new image\n",
    "img = encode(\"test_img.jpg\")\n",
    "img = img.reshape((1, 2048))\n",
    "\n",
    "caption = [vectorizer([\"startseq\"]).numpy()[0][0]]\n",
    "\n",
    "for i in range(max_length):\n",
    "    padded_caption = pad_sequences([caption], maxlen=22, padding='post')\n",
    "    prediction = model.predict([img, padded_caption], verbose=0)\n",
    "    word_index = np.argmax(prediction)\n",
    "    caption.append(word_index)\n",
    "    if word_index == vectorizer_word_index[\"endseq\"]:\n",
    "        break\n",
    "\n",
    "\n",
    "caption_words = [vectorizer_index_word[i] for i in caption]\n",
    "# join the words to form a sentence\n",
    "caption_sentence = ' '.join(caption_words[1:-1])\n",
    "caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beam_search_caption(image_path, beam_width):\n",
    "    # encode the image\n",
    "    image_vec = encode(image_path)\n",
    "    image_vec = image_vec.reshape(1, -1)\n",
    "    # initialize the caption with the start token\n",
    "    caption = [vectorizer([\"startseq\"]).numpy()[0][0]]\n",
    "    # initialize beam search\n",
    "    beam = [(caption, 0)]\n",
    "    \n",
    "    # loop until the end token or the maximum length is reached\n",
    "    for i in range(max_length):\n",
    "        # generate new candidates\n",
    "        candidates = []\n",
    "        for j in range(len(beam)):\n",
    "            seq, score = beam[j]\n",
    "            # check if the sequence ends with endseq\n",
    "            if seq[-1] == vectorizer_word_index[\"endseq\"]:\n",
    "                candidates.append((seq, score))\n",
    "                continue\n",
    "            # predict the next word using the model\n",
    "            padded_caption = pad_sequences([seq], maxlen=max_length, padding='post')\n",
    "            prediction = model.predict([image_vec, padded_caption], verbose=0)[0]\n",
    "            # get the top k words with the highest probability\n",
    "            top_k = prediction.argsort()[-beam_width:][::-1]\n",
    "            # add new candidates to the list\n",
    "            for w in top_k:\n",
    "                new_seq = seq + [w]\n",
    "                new_score = score + np.log(prediction[w])\n",
    "                candidates.append((new_seq, new_score))\n",
    "        # select top k candidates\n",
    "        beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "    # select the best candidate\n",
    "    seq, score = beam[0]\n",
    "    # convert the caption indices to words\n",
    "    caption_words = [vectorizer_index_word[i] for i in seq]\n",
    "    # join the words to form a sentence\n",
    "    caption_sentence = ' '.join(caption_words[1:-1])\n",
    "    return caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "8d0r5UjhV8Q-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a black and white dog is attempting to catch a yellow object\n"
     ]
    }
   ],
   "source": [
    "generated_caption = beam_search_caption(\"test_img.jpg\", 10)\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "7owjVv1BmGZb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 1174,    1,   52])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer([\"hey you what's up\"]).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3    2   41   21  124   67    2  195 2249    4]\n",
      "startseq a little girl climbing into a wooden playhouse . endseq\n",
      "[   3    2   41   21  124    6  383   20   63 2249    4]\n",
      "startseq a little girl climbing the stairs to her playhouse . endseq\n",
      "[   3    2   41   21    5    2   96  177  312   67    2  195 3010    4]\n",
      "startseq a little girl in a pink dress going into a wooden cabin . endseq\n"
     ]
    }
   ],
   "source": [
    "for i, row in train_df[2:5].iterrows():\n",
    "    seq = vectorizer(row['caption']).numpy()\n",
    "    print(vectorizer(row['caption']).numpy())\n",
    "    print(row['caption'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[0.32615873, 0.26416457, 0.20139042, ..., 0.33041683, 0.7433551 ,\n",
       "          0.18421653],\n",
       "         [0.32615873, 0.26416457, 0.20139042, ..., 0.33041683, 0.7433551 ,\n",
       "          0.18421653],\n",
       "         [0.32615873, 0.26416457, 0.20139042, ..., 0.33041683, 0.7433551 ,\n",
       "          0.18421653],\n",
       "         ...,\n",
       "         [0.43854874, 0.26131913, 0.41397506, ..., 0.35190374, 0.4639541 ,\n",
       "          0.17321709],\n",
       "         [0.43854874, 0.26131913, 0.41397506, ..., 0.35190374, 0.4639541 ,\n",
       "          0.17321709],\n",
       "         [0.43854874, 0.26131913, 0.41397506, ..., 0.35190374, 0.4639541 ,\n",
       "          0.17321709]], dtype=float32),\n",
       "  array([[  0,   0,   0, ...,   0,   0,   3],\n",
       "         [  0,   0,   0, ...,   0,   3,   2],\n",
       "         [  0,   0,   0, ...,   3,   2,  10],\n",
       "         ...,\n",
       "         [  0,   0,   0, ...,  88,  20,   2],\n",
       "         [  0,   0,   0, ...,  20,   2,  32],\n",
       "         [  0,   0,   0, ...,   2,  32, 179]], dtype=int32)],\n",
       " array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cffdeMRmHEb"
   },
   "source": [
    "# Model 2, more complicated\n",
    "* You can use a bidirectional LSTM instead of a single LSTM for the caption encoder. This way, you can capture the context from both directions of the caption sequence, and generate more coherent captions.\n",
    "\n",
    "* You can use an attention mechanism to allow the decoder to focus on different parts of the image and the caption encoder outputs at each time step. This way, you can generate more relevant and informative captions that align with the image content.\n",
    "\n",
    "* You can use a scheduled sampling technique to train the decoder with a mix of ground truth and predicted words. This way, you can reduce the exposure bias and improve the generalization ability of the decoder.\n",
    "\n",
    "* You can use a beam search instead of a greedy search for generating captions. This way, you can explore more possible captions and choose the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93vaiXH4mJx5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "# define the model\n",
    "ip1 = layers.Input(shape = (2048, ))\n",
    "fe1 = layers.Dropout(0.2)(ip1)\n",
    "fe2 = layers.Dense(256, activation = 'relu')(fe1)\n",
    "ip2 = layers.Input(shape = (max_length, ))\n",
    "se1 = layers.Embedding(vocab_size, embedding_dim, mask_zero = True)(ip2)\n",
    "se2 = layers.Dropout(0.2)(se1)\n",
    "se3 = Bidirectional(layers.LSTM(256, return_sequences=True))(se2) # use bidirectional LSTM\n",
    "decoder1 = layers.add([fe2, se3[:, -1]]) # use last hidden state of bidirectional LSTM\n",
    "decoder2 = layers.Dense(256, activation = 'relu')(decoder1)\n",
    "attn_layer = Attention() # use attention layer\n",
    "context_vector, attention_weights = attn_layer([decoder2, se3]) # get context vector and attention weights\n",
    "decoder3 = layers.Dense(256, activation='relu')(context_vector) # use context vector for final dense layer\n",
    "outputs = layers.Dense(vocab_size, activation = 'softmax')(decoder3)\n",
    "model_v2 = Model(inputs = [ip1, ip2], outputs = outputs)\n",
    "\n",
    "model_v2.layers[3].set_weights([embedding_matrix])\n",
    "model_v2.layers[3].trainable = False\n",
    "model_v2.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "num_batches = len(df_txt0) // batch_size\n",
    "model_v2.fit(train_generator, epochs = 50, steps_per_epoch=num_batches, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UXfPrCcmxsq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ6uH3DimysF"
   },
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgdmGhyLmy8M"
   },
   "outputs": [],
   "source": [
    "def beam_search(image_path, beam_size):\n",
    "  # encode the image\n",
    "  image_vec = encode(image_path)\n",
    "  # add another dimension to match the model input\n",
    "  image_vec = np.expand_dims(image_vec, axis=0)\n",
    "  # initialize the candidates with the start token\n",
    "  candidates = [[tokenizer.word_index['<start>']]]\n",
    "  # initialize the probabilities with 1\n",
    "  probabilities = [1]\n",
    "  # loop until the maximum length is reached\n",
    "  for i in range(max_length):\n",
    "    # initialize a list to store the next candidates\n",
    "    next_candidates = []\n",
    "    # initialize a list to store the next probabilities\n",
    "    next_probabilities = []\n",
    "    # loop over the current candidates\n",
    "    for j in range(len(candidates)):\n",
    "      # get the current candidate\n",
    "      candidate = candidates[j]\n",
    "      # pad the candidate sequence\n",
    "      padded_candidate = pad_sequences([candidate], maxlen=max_length, padding='post')\n",
    "      # predict the next word using the model\n",
    "      prediction = model.predict([image_vec, padded_candidate], verbose=0)\n",
    "      # get the top beam_size words and their probabilities\n",
    "      top_words = np.argsort(prediction[0])[-beam_size:]\n",
    "      top_probs = prediction[0][top_words]\n",
    "      # loop over the top words and their probabilities\n",
    "      for k in range(beam_size):\n",
    "        # get the word and its probability\n",
    "        word = top_words[k]\n",
    "        prob = top_probs[k]\n",
    "        # append the word to the candidate and multiply the probability\n",
    "        next_candidate = candidate + [word]\n",
    "        next_prob = probabilities[j] * prob\n",
    "        # append the next candidate and probability to the lists\n",
    "        next_candidates.append(next_candidate)\n",
    "        next_probabilities.append(next_prob)\n",
    "    # sort the next candidates and probabilities by descending order of probability\n",
    "    sorted_indices = np.argsort(next_probabilities)[::-1]\n",
    "    sorted_candidates = [next_candidates[i] for i in sorted_indices]\n",
    "    sorted_probabilities = [next_probabilities[i] for i in sorted_indices]\n",
    "    # select the top beam_size candidates and probabilities for the next iteration\n",
    "    candidates = sorted_candidates[:beam_size]\n",
    "    probabilities = sorted_probabilities[:beam_size]\n",
    "    # check if any candidate has reached the end token\n",
    "    end_index = tokenizer.word_index['<end>']\n",
    "    if any(candidate[-1] == end_index for candidate in candidates):\n",
    "      break\n",
    "  # return the candidate with the highest probability\n",
    "  best_candidate = candidates[0]\n",
    "  # convert the candidate indices to words\n",
    "  caption_words = [tokenizer.index_word[i] for i in best_candidate]\n",
    "  # join the words to form a sentence\n",
    "  caption_sentence = ' '.join(caption_words[1:-1])\n",
    "  return caption_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSA86kO8z39g"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(df, image_data, batch_size):\n",
    "    # create a dataset from the data frame\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df['filename'], df['caption']))\n",
    "    # map the filename to the image data\n",
    "    ds = ds.map(lambda x, y: (image_data[x], y))\n",
    "    # apply the TextVectorization layer as a transformation\n",
    "    ds = ds.map(lambda x, y: (x, vectorizer([y])))\n",
    "    # unbatch the dataset to get individual elements\n",
    "    ds = ds.unbatch()\n",
    "    # create input and output sequences\n",
    "    ds = ds.map(lambda x, y: (x, y[:-1], y[1:]))\n",
    "    # pad and one-hot encode the sequences if needed\n",
    "    # ds = ds.map(lambda x, y, z: (x, pad_sequences([y], maxlen=max_length)[0], to_categorical([z], num_classes=vocab_size)[0]))\n",
    "    # batch the dataset\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "batch_size = 64\n",
    "train_generator = data_generator(df_txt, image_data, batch_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
